{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "class BrightDataLinkedInScraper:\n",
        "    def __init__(self, api_token: str, dataset_id: str = \"gd_l1viktl72bvl7bjuj0\"):\n",
        "        \"\"\"\n",
        "        Initialize with Bright Data API token and your LinkedIn scraper dataset ID\n",
        "\n",
        "        Args:\n",
        "            api_token: Your Bright Data API token\n",
        "            dataset_id: Your LinkedIn scraper dataset ID (e.g., gd_l1viktl72bvl7bjuj0)\n",
        "        \"\"\"\n",
        "        self.api_token = api_token\n",
        "        self.dataset_id = dataset_id\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        self.base_url = \"https://api.brightdata.com/datasets/v3\"\n",
        "\n",
        "    def trigger_scraping(self, profile_urls: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Trigger LinkedIn profile scraping using the CORRECT Bright Data API endpoint\n",
        "        Based on official documentation: /datasets/v3/trigger\n",
        "\n",
        "        Args:\n",
        "            profile_urls: List of LinkedIn profile URLs to scrape\n",
        "\n",
        "        Returns:\n",
        "            API response with job details\n",
        "        \"\"\"\n",
        "        # Format URLs as required by Bright Data API\n",
        "        url_data = []\n",
        "        for url in profile_urls:\n",
        "            url_data.append({\"url\": url})\n",
        "\n",
        "        # Use the correct trigger endpoint from documentation\n",
        "        api_url = f\"{self.base_url}/trigger\"\n",
        "        params = {\n",
        "            \"dataset_id\": self.dataset_id,\n",
        "            \"format\": \"json\",\n",
        "            \"uncompressed_webhook\": \"true\"\n",
        "        }\n",
        "\n",
        "        print(f\"üöÄ Triggering scraping job...\")\n",
        "        print(f\"API URL: {api_url}\")\n",
        "        print(f\"Dataset ID: {self.dataset_id}\")\n",
        "        print(f\"URLs to scrape: {len(profile_urls)}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                api_url,\n",
        "                headers=self.headers,\n",
        "                json=url_data,\n",
        "                params=params\n",
        "            )\n",
        "\n",
        "            print(f\"Response status: {response.status_code}\")\n",
        "\n",
        "            if response.status_code in [200, 201, 202]:\n",
        "                result = response.json()\n",
        "                print(f\"‚úÖ Scraping job triggered successfully!\")\n",
        "                print(f\"Response: {result}\")\n",
        "                return result\n",
        "            else:\n",
        "                print(f\"‚ùå Request failed: {response.status_code}\")\n",
        "                print(f\"Response text: {response.text}\")\n",
        "                return {\"error\": f\"HTTP {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error triggering scraping: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def check_job_progress(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Check the progress of scraping jobs\n",
        "        Note: The /progress/ endpoint might not work, so we'll use snapshots as alternative\n",
        "        \"\"\"\n",
        "        # Try the progress endpoint first\n",
        "        progress_url = f\"{self.base_url}/progress/\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(progress_url, headers=self.headers)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "        except Exception as e:\n",
        "            print(f\"Progress endpoint failed: {e}\")\n",
        "\n",
        "        # Fallback: Check snapshots for status\n",
        "        print(\"Checking snapshots instead...\")\n",
        "        return self.get_snapshots(\"running\")\n",
        "\n",
        "    def get_snapshots(self, status: str = \"ready\") -> Dict:\n",
        "        \"\"\"\n",
        "        Get snapshots with specific status\n",
        "\n",
        "        Args:\n",
        "            status: 'ready', 'running', 'failed', etc.\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/snapshots\"\n",
        "        params = {\n",
        "            \"dataset_id\": self.dataset_id,\n",
        "            \"status\": status\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                print(f\"Snapshots request failed: {response.status_code}\")\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting snapshots: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def download_snapshot(self, snapshot_id: str = None, format_type: str = \"json\") -> Optional[List[Dict]]:\n",
        "        \"\"\"\n",
        "        Download scraped data using CORRECT Bright Data API endpoints\n",
        "\n",
        "        Based on official documentation:\n",
        "        - https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json (for specific snapshot)\n",
        "        - https://api.brightdata.com/datasets/snapshots/{snapshot_id}/download (alternative endpoint)\n",
        "\n",
        "        Args:\n",
        "            snapshot_id: Specific snapshot ID, or None for latest\n",
        "            format_type: Data format (json, csv)\n",
        "        \"\"\"\n",
        "        if snapshot_id:\n",
        "            # Method 1: Use the official documented endpoint\n",
        "            url = f\"{self.base_url}/snapshot/{snapshot_id}\"\n",
        "            params = {\"format\": format_type}\n",
        "\n",
        "            print(f\"üì° Downloading from: {url}\")\n",
        "            try:\n",
        "                response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    print(f\"‚úÖ Successfully downloaded data!\")\n",
        "\n",
        "                    # Handle different response formats\n",
        "                    if isinstance(data, list):\n",
        "                        return data\n",
        "                    elif isinstance(data, dict):\n",
        "                        if 'data' in data:\n",
        "                            return data['data']\n",
        "                        elif 'results' in data:\n",
        "                            return data['results']\n",
        "                        else:\n",
        "                            return [data]\n",
        "                    return []\n",
        "\n",
        "                elif response.status_code == 202:\n",
        "                    print(\"‚è≥ Snapshot still processing... waiting a bit longer\")\n",
        "                    return None\n",
        "\n",
        "                else:\n",
        "                    print(f\"‚ùå Download failed: {response.status_code} - {response.text}\")\n",
        "\n",
        "                    # Method 2: Try alternative endpoint from docs\n",
        "                    alt_url = f\"https://api.brightdata.com/datasets/snapshots/{snapshot_id}/download\"\n",
        "                    print(f\"üîÑ Trying alternative endpoint: {alt_url}\")\n",
        "\n",
        "                    response = requests.get(alt_url, headers=self.headers, params=params)\n",
        "                    if response.status_code == 200:\n",
        "                        data = response.json()\n",
        "                        return data if isinstance(data, list) else [data]\n",
        "                    else:\n",
        "                        print(f\"‚ùå Alternative endpoint also failed: {response.status_code}\")\n",
        "                        return None\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error downloading data: {e}\")\n",
        "                return None\n",
        "\n",
        "        else:\n",
        "            # Try to get latest ready snapshot first\n",
        "            ready_snapshots = self.get_snapshots(\"ready\")\n",
        "\n",
        "            # Extract snapshot ID from the response\n",
        "            snapshot_ids = []\n",
        "            if isinstance(ready_snapshots, dict) and ready_snapshots.get('data'):\n",
        "                # Look for 'id' field in snapshots\n",
        "                snapshot_ids = [snap.get('id') or snap.get('snapshot_id') for snap in ready_snapshots['data']]\n",
        "            elif isinstance(ready_snapshots, list):\n",
        "                snapshot_ids = [snap.get('id') or snap.get('snapshot_id') for snap in ready_snapshots]\n",
        "\n",
        "            # Filter out None values\n",
        "            snapshot_ids = [sid for sid in snapshot_ids if sid]\n",
        "\n",
        "            if snapshot_ids:\n",
        "                # Use the most recent snapshot\n",
        "                latest_snapshot_id = snapshot_ids[0]\n",
        "                print(f\"üì• Using latest snapshot ID: {latest_snapshot_id}\")\n",
        "                return self.download_snapshot(latest_snapshot_id, format_type)\n",
        "            else:\n",
        "                print(\"‚ùå No snapshot IDs found in ready snapshots\")\n",
        "                return None\n",
        "\n",
        "    def wait_for_specific_completion(self, snapshot_id: str, max_wait: int = 120, check_interval: int = 10) -> bool:\n",
        "        \"\"\"\n",
        "        Wait for a SPECIFIC scraping job to complete using its snapshot ID\n",
        "\n",
        "        Args:\n",
        "            snapshot_id: The specific snapshot ID to wait for\n",
        "            max_wait: Maximum wait time in seconds (reduced to 2 minutes)\n",
        "            check_interval: Check interval in seconds\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        print(f\"‚è≥ Waiting for specific job {snapshot_id} to complete...\")\n",
        "        print(f\"üí° Will timeout after {max_wait} seconds\")\n",
        "\n",
        "        attempts = 0\n",
        "        while time.time() - start_time < max_wait:\n",
        "            attempts += 1\n",
        "\n",
        "            # Try to download the specific snapshot to see if it's ready\n",
        "            url = f\"{self.base_url}/snapshot/{snapshot_id}\"\n",
        "            params = {\"format\": \"json\"}\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=self.headers, params=params)\n",
        "                elapsed = int(time.time() - start_time)\n",
        "\n",
        "                print(f\"Attempt {attempts}: Status {response.status_code} ({elapsed}s elapsed)\")\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    print(f\"‚úÖ Job {snapshot_id} is ready!\")\n",
        "                    return True\n",
        "                elif response.status_code == 202:\n",
        "                    print(f\"‚è≥ Job still processing...\")\n",
        "                elif response.status_code == 404:\n",
        "                    print(f\"‚è≥ Job not found yet, still initializing...\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Unexpected status: {response.status_code}\")\n",
        "                    print(f\"Response: {response.text[:200]}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                elapsed = int(time.time() - start_time)\n",
        "                print(f\"‚è≥ Error checking job status ({elapsed}s elapsed): {e}\")\n",
        "\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        print(f\"‚è∞ Timeout reached after {max_wait} seconds\")\n",
        "        return False\n",
        "\n",
        "    def fallback_download_latest(self) -> Optional[List[Dict]]:\n",
        "        \"\"\"\n",
        "        Fallback method to download the most recent data if specific job fails\n",
        "        \"\"\"\n",
        "        print(\"üîÑ Falling back to download latest available data...\")\n",
        "\n",
        "        # Get all ready snapshots\n",
        "        ready_snapshots = self.get_snapshots(\"ready\")\n",
        "\n",
        "        if isinstance(ready_snapshots, dict) and ready_snapshots.get('data'):\n",
        "            snapshots = ready_snapshots['data']\n",
        "        elif isinstance(ready_snapshots, list):\n",
        "            snapshots = ready_snapshots\n",
        "        else:\n",
        "            print(\"‚ùå No snapshots found\")\n",
        "            return None\n",
        "\n",
        "        if not snapshots:\n",
        "            print(\"‚ùå No ready snapshots available\")\n",
        "            return None\n",
        "\n",
        "        # Sort by creation date and get the most recent\n",
        "        try:\n",
        "            snapshots_sorted = sorted(snapshots, key=lambda x: x.get('created', ''), reverse=True)\n",
        "            latest_snapshot = snapshots_sorted[0]\n",
        "            latest_id = latest_snapshot.get('id')\n",
        "\n",
        "            print(f\"üì• Trying latest snapshot: {latest_id}\")\n",
        "            print(f\"   Created: {latest_snapshot.get('created', 'unknown')}\")\n",
        "\n",
        "            return self.download_snapshot(latest_id)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error with fallback download: {e}\")\n",
        "            return None\n",
        "\n",
        "def scrape_linkedin_profiles_complete(api_token: str, dataset_id: str, profile_urls: List[str]) -> Optional[List[Dict]]:\n",
        "    \"\"\"\n",
        "    Complete LinkedIn scraping workflow using correct Bright Data API\n",
        "\n",
        "    Args:\n",
        "        api_token: Your Bright Data API token\n",
        "        dataset_id: Your dataset ID (e.g., gd_l1viktl72bvl7bjuj0)\n",
        "        profile_urls: List of LinkedIn profile URLs to scrape\n",
        "\n",
        "    Returns:\n",
        "        Scraped profile data\n",
        "    \"\"\"\n",
        "    scraper = BrightDataLinkedInScraper(api_token, dataset_id)\n",
        "\n",
        "    print(\"üîç BRIGHT DATA LINKEDIN SCRAPER\")\n",
        "    print(\"Using OFFICIAL API endpoints from documentation\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Step 1: Trigger scraping\n",
        "    print(f\"\\n1Ô∏è‚É£ Triggering scraping for {len(profile_urls)} profiles...\")\n",
        "    for i, url in enumerate(profile_urls, 1):\n",
        "        print(f\"   {i}. {url}\")\n",
        "\n",
        "    trigger_result = scraper.trigger_scraping(profile_urls)\n",
        "\n",
        "    if trigger_result.get(\"error\"):\n",
        "        print(\"‚ùå Failed to trigger scraping job\")\n",
        "        return None\n",
        "\n",
        "    # Get the specific snapshot ID from the trigger response\n",
        "    new_snapshot_id = trigger_result.get(\"snapshot_id\")\n",
        "    print(f\"üéØ New job snapshot ID: {new_snapshot_id}\")\n",
        "\n",
        "    # Step 2: Wait for SPECIFIC job completion (with shorter timeout)\n",
        "    print(f\"\\n2Ô∏è‚É£ Waiting for the specific job to complete...\")\n",
        "    job_completed = scraper.wait_for_specific_completion(new_snapshot_id, max_wait=120)\n",
        "\n",
        "    if job_completed:\n",
        "        # Step 3: Download results from the SPECIFIC snapshot\n",
        "        print(f\"\\n3Ô∏è‚É£ Downloading data from specific job: {new_snapshot_id}\")\n",
        "        results = scraper.download_snapshot(new_snapshot_id)\n",
        "\n",
        "        if results:\n",
        "            print(f\"‚úÖ Successfully downloaded {len(results)} profiles from the new job!\")\n",
        "            return results\n",
        "        else:\n",
        "            print(\"‚ùå No data found in the specific job\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Specific job did not complete in time\")\n",
        "        print(\"üí° Job might still be running in the background\")\n",
        "\n",
        "    # Fallback: Try to get the most recent data available\n",
        "    print(f\"\\nüîÑ FALLBACK: Trying to get most recent available data...\")\n",
        "    fallback_results = scraper.fallback_download_latest()\n",
        "\n",
        "    if fallback_results:\n",
        "        print(f\"‚úÖ Downloaded {len(fallback_results)} profiles from latest snapshot\")\n",
        "        print(\"‚ö†Ô∏è  Note: This might be from a previous job, not the one just triggered\")\n",
        "        return fallback_results\n",
        "    else:\n",
        "        print(\"‚ùå No data available at all\")\n",
        "        return None\n",
        "\n",
        "def display_profile_data(profiles: List[Dict]):\n",
        "    \"\"\"Display scraped profile data in a nice format\"\"\"\n",
        "    if not profiles:\n",
        "        print(\"No profiles to display\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüìã SCRAPED LINKEDIN PROFILES ({len(profiles)} total)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, profile in enumerate(profiles[:3], 1):  # Show first 3 profiles\n",
        "        print(f\"\\nüë§ PROFILE {i}:\")\n",
        "        print(f\"   Name: {profile.get('name', 'N/A')}\")\n",
        "        print(f\"   LinkedIn ID: {profile.get('linkedin_id', 'N/A')}\")\n",
        "        print(f\"   Location: {profile.get('city', 'N/A')}\")\n",
        "        print(f\"   Current Company: {profile.get('current_company_name', 'N/A')}\")\n",
        "        print(f\"   Position: {profile.get('position', 'N/A')}\")\n",
        "        print(f\"   Followers: {profile.get('followers', 'N/A')}\")\n",
        "        print(f\"   Connections: {profile.get('connections', 'N/A')}\")\n",
        "\n",
        "        # About section (truncated)\n",
        "        about = profile.get('about', '')\n",
        "        if about and len(about) > 0:\n",
        "            about_preview = about[:150] + \"...\" if len(about) > 150 else about\n",
        "            print(f\"   About: {about_preview}\")\n",
        "\n",
        "        # Education\n",
        "        if profile.get('education') and len(profile['education']) > 0:\n",
        "            edu = profile['education'][0]\n",
        "            print(f\"   Education: {edu.get('title', 'N/A')}\")\n",
        "\n",
        "        # Experience\n",
        "        if profile.get('experience') and len(profile['experience']) > 0:\n",
        "            exp = profile['experience'][0]\n",
        "            print(f\"   Experience: {exp.get('title', 'N/A')} at {exp.get('company', 'N/A')}\")\n",
        "\n",
        "        print(f\"   Profile URL: {profile.get('url', 'N/A')}\")\n",
        "\n",
        "    if len(profiles) > 3:\n",
        "        print(f\"\\n... and {len(profiles) - 3} more profiles\")\n",
        "\n",
        "def main():\n",
        "    # Configuration - Replace with your actual values\n",
        "    API_TOKEN = \"\"\n",
        "    DATASET_ID = \"gd_l1viktl72bvl7bjuj0\"  # Your LinkedIn scraper dataset ID\n",
        "\n",
        "    # URLs to scrape\n",
        "    profile_urls = [\n",
        "        \"https://www.linkedin.com/in/ketanarun/\",\n",
        "        \"https://www.linkedin.com/in/aravind-srinivas-16051987/\"\n",
        "        # Add more URLs here as needed\n",
        "    ]\n",
        "\n",
        "    # Run complete scraping workflow\n",
        "    results = scrape_linkedin_profiles_complete(API_TOKEN, DATASET_ID, profile_urls)\n",
        "\n",
        "    if results:\n",
        "        # Display the scraped data\n",
        "        display_profile_data(results)\n",
        "\n",
        "        # Save to file\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"linkedin_profiles_{timestamp}.json\"\n",
        "\n",
        "        data_package = {\n",
        "            \"metadata\": {\n",
        "                \"scraped_at\": datetime.now().isoformat(),\n",
        "                \"total_profiles\": len(results),\n",
        "                \"api_token_used\": API_TOKEN[-10:] + \"...\",  # Show last 10 chars for verification\n",
        "                \"dataset_id\": DATASET_ID\n",
        "            },\n",
        "            \"profiles\": results\n",
        "        }\n",
        "\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data_package, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nüíæ Data saved to: {filename}\")\n",
        "\n",
        "        # Show statistics\n",
        "        companies = [p.get('current_company_name') for p in results if p.get('current_company_name')]\n",
        "        locations = [p.get('city') for p in results if p.get('city')]\n",
        "\n",
        "        print(f\"\\nüìä QUICK STATS:\")\n",
        "        print(f\"   Total profiles: {len(results)}\")\n",
        "        print(f\"   Unique companies: {len(set(companies))}\")\n",
        "        print(f\"   Unique locations: {len(set(locations))}\")\n",
        "\n",
        "        if companies:\n",
        "            from collections import Counter\n",
        "            top_companies = Counter(companies).most_common(3)\n",
        "            print(f\"   Top companies: {[comp for comp, count in top_companies]}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ùå No data retrieved\")\n",
        "        print(\"\\nüîß TROUBLESHOOTING STEPS:\")\n",
        "        print(\"1. Verify your API token is correct\")\n",
        "        print(\"2. Check that dataset_id matches your scraper\")\n",
        "        print(\"3. Make sure URLs are valid LinkedIn profiles\")\n",
        "        print(\"4. Try with a smaller number of URLs first\")\n",
        "\n",
        "        # Test API connectivity\n",
        "        scraper = BrightDataLinkedInScraper(API_TOKEN, DATASET_ID)\n",
        "        print(\"\\nüß™ Testing API connectivity...\")\n",
        "        snapshots = scraper.get_snapshots(\"ready\")\n",
        "        if snapshots:\n",
        "            snapshot_count = 0\n",
        "            if isinstance(snapshots, dict) and snapshots.get('data'):\n",
        "                snapshot_count = len(snapshots['data'])\n",
        "            elif isinstance(snapshots, list):\n",
        "                snapshot_count = len(snapshots)\n",
        "            print(f\"‚úÖ API connection working - found {snapshot_count} ready snapshots\")\n",
        "        else:\n",
        "            print(\"‚ùå API connection issues\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUuKD4qe7Gxu",
        "outputId": "5f1413be-4ae6-4f4b-f250-0c71450101d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç BRIGHT DATA LINKEDIN SCRAPER\n",
            "Using OFFICIAL API endpoints from documentation\n",
            "=======================================================\n",
            "\n",
            "1Ô∏è‚É£ Triggering scraping for 2 profiles...\n",
            "   1. https://www.linkedin.com/in/ketanarun/\n",
            "   2. https://www.linkedin.com/in/aravind-srinivas-16051987/\n",
            "üöÄ Triggering scraping job...\n",
            "API URL: https://api.brightdata.com/datasets/v3/trigger\n",
            "Dataset ID: gd_l1viktl72bvl7bjuj0\n",
            "URLs to scrape: 2\n",
            "Response status: 200\n",
            "‚úÖ Scraping job triggered successfully!\n",
            "Response: {'snapshot_id': 'sd_mfxrzseufz76qseoo'}\n",
            "üéØ New job snapshot ID: sd_mfxrzseufz76qseoo\n",
            "\n",
            "2Ô∏è‚É£ Waiting for the specific job to complete...\n",
            "‚è≥ Waiting for specific job sd_mfxrzseufz76qseoo to complete...\n",
            "üí° Will timeout after 120 seconds\n",
            "Attempt 1: Status 202 (0s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 2: Status 202 (10s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 3: Status 202 (20s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 4: Status 200 (31s elapsed)\n",
            "‚úÖ Job sd_mfxrzseufz76qseoo is ready!\n",
            "\n",
            "3Ô∏è‚É£ Downloading data from specific job: sd_mfxrzseufz76qseoo\n",
            "üì° Downloading from: https://api.brightdata.com/datasets/v3/snapshot/sd_mfxrzseufz76qseoo\n",
            "‚úÖ Successfully downloaded data!\n",
            "‚úÖ Successfully downloaded 2 profiles from the new job!\n",
            "\n",
            "üìã SCRAPED LINKEDIN PROFILES (2 total)\n",
            "============================================================\n",
            "\n",
            "üë§ PROFILE 1:\n",
            "   Name: Ketan Arun Thatte\n",
            "   LinkedIn ID: ketanarun\n",
            "   Location: Prayagraj, Uttar Pradesh, India\n",
            "   Current Company: G\n",
            "   Position: Corporate Quality Head, Lead Auditor, Ex-SYSTRA, Ex-SMCC, Ex-L&T, Ex-JMB, Ex-TATA- 20+Yrs Exp in QAQC (Construction)\n",
            "   Followers: 1184\n",
            "   Connections: 500\n",
            "   About: Civil engineering Postgraduate with 20 + years of vivid experience in the field of QAQC & Execution of arrays of Civil Engg projects viz. Railways, Me...\n",
            "   Education: Maharaja Sayajirao University of Baroda, Vadodara\n",
            "   Experience: Corporate Quality Head at G\n",
            "   Profile URL: https://cl.linkedin.com/in/Ketanarun\n",
            "\n",
            "üë§ PROFILE 2:\n",
            "   Name: Aravind Srinivas\n",
            "   LinkedIn ID: aravind-srinivas-16051987\n",
            "   Location: San Francisco, California, United States\n",
            "   Current Company: Perplexity\n",
            "   Position: Cofounder, President & CEO, Perplexity\n",
            "   Followers: 627871\n",
            "   Connections: 500\n",
            "   About: Cofounder, President and CEO of Perplexity. Try it out at https://www.perplexity.ai\n",
            "   Education: University of California, Berkeley\n",
            "   Experience: Cofounder, President, CEO at Perplexity\n",
            "   Profile URL: https://my.linkedin.com/in/aravind-srinivas-16051987\n",
            "\n",
            "üíæ Data saved to: linkedin_profiles_20250924_092300.json\n",
            "\n",
            "üìä QUICK STATS:\n",
            "   Total profiles: 2\n",
            "   Unique companies: 2\n",
            "   Unique locations: 2\n",
            "   Top companies: ['G', 'Perplexity']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}