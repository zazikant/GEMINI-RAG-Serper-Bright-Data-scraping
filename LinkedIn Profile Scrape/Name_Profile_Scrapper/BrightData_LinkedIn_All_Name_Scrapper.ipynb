{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "class BrightDataLinkedInNameScraper:\n",
        "    def __init__(self, api_token: str, dataset_id: str = \"gd_l1viktl72bvl7bjuj0\"):\n",
        "        \"\"\"\n",
        "        Initialize LinkedIn name-based scraper with Bright Data API\n",
        "\n",
        "        Args:\n",
        "            api_token: Your Bright Data API token\n",
        "            dataset_id: Your LinkedIn scraper dataset ID\n",
        "        \"\"\"\n",
        "        self.api_token = api_token\n",
        "        self.dataset_id = dataset_id\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        self.base_url = \"https://api.brightdata.com/datasets/v3\"\n",
        "\n",
        "    def trigger_name_discovery(self, people: List[Dict[str, str]],\n",
        "                             additional_params: Optional[Dict] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Trigger LinkedIn profile discovery using names\n",
        "\n",
        "        Args:\n",
        "            people: List of dictionaries with 'first_name' and 'last_name'\n",
        "            additional_params: Optional additional search parameters (company, location, etc.)\n",
        "\n",
        "        Returns:\n",
        "            API response with job details including snapshot_id\n",
        "        \"\"\"\n",
        "        # Validate input data\n",
        "        for person in people:\n",
        "            if 'first_name' not in person or 'last_name' not in person:\n",
        "                return {\"error\": \"Each person must have 'first_name' and 'last_name'\"}\n",
        "\n",
        "        api_url = f\"{self.base_url}/trigger\"\n",
        "        params = {\n",
        "            \"dataset_id\": self.dataset_id,\n",
        "            \"include_errors\": \"true\",\n",
        "            \"type\": \"discover_new\",\n",
        "            \"discover_by\": \"name\"\n",
        "        }\n",
        "\n",
        "        # Add any additional search parameters\n",
        "        if additional_params:\n",
        "            params.update(additional_params)\n",
        "\n",
        "        print(f\"üîç Triggering LinkedIn name discovery for {len(people)} people...\")\n",
        "        for i, person in enumerate(people, 1):\n",
        "            name_display = f\"{person['first_name']} {person['last_name']}\"\n",
        "            # Add company/location if provided in person data\n",
        "            if 'company' in person:\n",
        "                name_display += f\" (Company: {person['company']})\"\n",
        "            if 'location' in person:\n",
        "                name_display += f\" (Location: {person['location']})\"\n",
        "            print(f\"   {i}. {name_display}\")\n",
        "\n",
        "        print(f\"API URL: {api_url}\")\n",
        "        print(f\"Dataset ID: {self.dataset_id}\")\n",
        "        print(f\"Search parameters: {params}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=self.headers, json=people, params=params)\n",
        "\n",
        "            print(f\"Response status: {response.status_code}\")\n",
        "\n",
        "            if response.status_code in [200, 201, 202]:\n",
        "                result = response.json()\n",
        "                print(f\"‚úÖ Name discovery job triggered successfully!\")\n",
        "                print(f\"Snapshot ID: {result.get('snapshot_id')}\")\n",
        "                return result\n",
        "            else:\n",
        "                print(f\"‚ùå Request failed: {response.status_code}\")\n",
        "                print(f\"Response: {response.text}\")\n",
        "                return {\"error\": f\"HTTP {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error triggering discovery: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def wait_for_completion(self, snapshot_id: str, max_wait: int = 600, check_interval: int = 20) -> bool:\n",
        "        \"\"\"\n",
        "        Wait for a name discovery job to complete\n",
        "        Name discovery typically takes longer than URL scraping\n",
        "\n",
        "        Args:\n",
        "            snapshot_id: The snapshot ID to wait for\n",
        "            max_wait: Maximum wait time in seconds (default 10 minutes)\n",
        "            check_interval: Check interval in seconds\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        print(f\"‚è≥ Waiting for discovery job {snapshot_id} to complete...\")\n",
        "        print(f\"üí° Discovery jobs may take longer - timeout after {max_wait} seconds\")\n",
        "\n",
        "        attempts = 0\n",
        "        while time.time() - start_time < max_wait:\n",
        "            attempts += 1\n",
        "\n",
        "            url = f\"{self.base_url}/snapshot/{snapshot_id}\"\n",
        "            params = {\"format\": \"json\"}\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=self.headers, params=params)\n",
        "                elapsed = int(time.time() - start_time)\n",
        "\n",
        "                print(f\"Attempt {attempts}: Status {response.status_code} ({elapsed}s elapsed)\")\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    print(f\"‚úÖ Discovery job {snapshot_id} is ready!\")\n",
        "                    return True\n",
        "                elif response.status_code == 202:\n",
        "                    print(f\"‚è≥ Job still processing... (discovery can take several minutes)\")\n",
        "                elif response.status_code == 404:\n",
        "                    print(f\"‚è≥ Job not found yet, still initializing...\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Unexpected status: {response.status_code}\")\n",
        "                    print(f\"Response: {response.text[:200]}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                elapsed = int(time.time() - start_time)\n",
        "                print(f\"‚è≥ Error checking job status ({elapsed}s elapsed): {e}\")\n",
        "\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        print(f\"‚è∞ Timeout reached after {max_wait} seconds\")\n",
        "        print(f\"üí° Job may still be running - you can check later with snapshot ID: {snapshot_id}\")\n",
        "        return False\n",
        "\n",
        "    def download_results(self, snapshot_id: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"\n",
        "        Download discovered profile data from a completed job\n",
        "\n",
        "        Args:\n",
        "            snapshot_id: The snapshot ID to download from\n",
        "\n",
        "        Returns:\n",
        "            List of discovered profiles or None if failed\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/snapshot/{snapshot_id}\"\n",
        "        params = {\"format\": \"json\"}\n",
        "\n",
        "        print(f\"üì° Downloading discovery results from snapshot: {snapshot_id}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                print(f\"‚úÖ Successfully downloaded discovery data!\")\n",
        "\n",
        "                # Handle different response formats\n",
        "                if isinstance(data, list):\n",
        "                    return data\n",
        "                elif isinstance(data, dict):\n",
        "                    if 'data' in data:\n",
        "                        return data['data']\n",
        "                    elif 'results' in data:\n",
        "                        return data['results']\n",
        "                    else:\n",
        "                        return [data]\n",
        "                return []\n",
        "\n",
        "            elif response.status_code == 202:\n",
        "                print(\"‚è≥ Snapshot still processing... try again later\")\n",
        "                return None\n",
        "            else:\n",
        "                print(f\"‚ùå Download failed: {response.status_code}\")\n",
        "                print(f\"Response: {response.text[:200]}...\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading results: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_snapshots(self, status: str = \"ready\") -> Dict:\n",
        "        \"\"\"Get snapshots with specific status for troubleshooting\"\"\"\n",
        "        url = f\"{self.base_url}/snapshots\"\n",
        "        params = {\n",
        "            \"dataset_id\": self.dataset_id,\n",
        "            \"status\": status\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                print(f\"Snapshots request failed: {response.status_code}\")\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting snapshots: {e}\")\n",
        "            return {}\n",
        "\n",
        "\n",
        "def discover_linkedin_profiles_by_names(api_token: str, dataset_id: str,\n",
        "                                       people: List[Dict[str, str]],\n",
        "                                       additional_params: Optional[Dict] = None) -> Optional[List[Dict]]:\n",
        "    \"\"\"\n",
        "    Complete LinkedIn profile discovery workflow using names\n",
        "\n",
        "    Args:\n",
        "        api_token: Your Bright Data API token\n",
        "        dataset_id: Your dataset ID\n",
        "        people: List of dictionaries with 'first_name' and 'last_name'\n",
        "                Can also include 'company', 'location' for better targeting\n",
        "        additional_params: Optional global search parameters\n",
        "\n",
        "    Returns:\n",
        "        List of discovered profile data or None if failed\n",
        "    \"\"\"\n",
        "    scraper = BrightDataLinkedInNameScraper(api_token, dataset_id)\n",
        "\n",
        "    print(\"üîç BRIGHT DATA LINKEDIN NAME DISCOVERY\")\n",
        "    print(\"Using OFFICIAL API endpoints for name-based search\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Trigger discovery\n",
        "    trigger_result = scraper.trigger_name_discovery(people, additional_params)\n",
        "\n",
        "    if trigger_result.get(\"error\"):\n",
        "        print(\"‚ùå Failed to trigger discovery job\")\n",
        "        print(f\"Error details: {trigger_result}\")\n",
        "        return None\n",
        "\n",
        "    snapshot_id = trigger_result.get(\"snapshot_id\")\n",
        "    if not snapshot_id:\n",
        "        print(\"‚ùå No snapshot ID received from API\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üéØ Discovery job started with snapshot ID: {snapshot_id}\")\n",
        "\n",
        "    # Wait for completion (discovery jobs take longer than URL scraping)\n",
        "    print(f\"\\n‚è≥ WAITING FOR DISCOVERY COMPLETION...\")\n",
        "    job_completed = scraper.wait_for_completion(snapshot_id)\n",
        "\n",
        "    if job_completed:\n",
        "        # Download results\n",
        "        print(f\"\\nüì• DOWNLOADING RESULTS...\")\n",
        "        results = scraper.download_results(snapshot_id)\n",
        "\n",
        "        if results:\n",
        "            print(f\"‚úÖ Successfully discovered {len(results)} profiles!\")\n",
        "            return results\n",
        "        else:\n",
        "            print(\"‚ùå No profiles discovered or download failed\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"‚ùå Discovery job did not complete within timeout\")\n",
        "        print(f\"üí° You can manually check this snapshot later: {snapshot_id}\")\n",
        "\n",
        "        # Try to download anyway in case it completed after timeout\n",
        "        print(f\"\\nüîÑ Attempting download anyway...\")\n",
        "        results = scraper.download_results(snapshot_id)\n",
        "        if results:\n",
        "            print(f\"‚úÖ Found {len(results)} profiles after timeout!\")\n",
        "            return results\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "def filter_quality_profiles(profiles: List[Dict], min_quality_score: int = 3) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Filter profiles to keep only those with sufficient data quality\n",
        "\n",
        "    Args:\n",
        "        profiles: List of discovered profiles\n",
        "        min_quality_score: Minimum quality score (1-10, higher = better)\n",
        "\n",
        "    Returns:\n",
        "        Filtered list of high-quality profiles\n",
        "    \"\"\"\n",
        "    if not profiles:\n",
        "        return []\n",
        "\n",
        "    quality_profiles = []\n",
        "\n",
        "    for profile in profiles:\n",
        "        quality_score = 0\n",
        "\n",
        "        # Score based on available data\n",
        "        if profile.get('name') and len(profile.get('name', '')) > 3:\n",
        "            quality_score += 1\n",
        "        if profile.get('current_company_name') and profile.get('current_company_name') not in ['N/A', '--', None]:\n",
        "            quality_score += 2\n",
        "        if profile.get('position') and profile.get('position') not in ['N/A', '--', None]:\n",
        "            quality_score += 2\n",
        "        if profile.get('about') and len(profile.get('about', '')) > 50:\n",
        "            quality_score += 2\n",
        "        if profile.get('experience') and len(profile.get('experience', [])) > 0:\n",
        "            quality_score += 1\n",
        "        if profile.get('education') and len(profile.get('education', [])) > 0:\n",
        "            quality_score += 1\n",
        "        if profile.get('followers') and str(profile.get('followers', '')).isdigit():\n",
        "            quality_score += 1\n",
        "\n",
        "        # Add profile with quality score\n",
        "        profile['_quality_score'] = quality_score\n",
        "\n",
        "        if quality_score >= min_quality_score:\n",
        "            quality_profiles.append(profile)\n",
        "\n",
        "    # Sort by quality score (highest first)\n",
        "    quality_profiles.sort(key=lambda x: x.get('_quality_score', 0), reverse=True)\n",
        "\n",
        "    return quality_profiles\n",
        "\n",
        "def display_discovered_profiles(profiles: List[Dict], max_display: int = 5, show_quality_filter: bool = True):\n",
        "    \"\"\"Display discovered profile data in a formatted way with quality filtering\"\"\"\n",
        "    if not profiles:\n",
        "        print(\"No profiles to display\")\n",
        "        return\n",
        "\n",
        "    # Filter for quality profiles first\n",
        "    if show_quality_filter:\n",
        "        quality_profiles = filter_quality_profiles(profiles, min_quality_score=3)\n",
        "        empty_profiles = [p for p in profiles if p not in quality_profiles]\n",
        "\n",
        "        print(f\"\\nüìã LINKEDIN PROFILE ANALYSIS\")\n",
        "        print(\"=\" * 65)\n",
        "        print(f\"   Total profiles found: {len(profiles)}\")\n",
        "        print(f\"   High-quality profiles: {len(quality_profiles)}\")\n",
        "        print(f\"   Low-quality/empty profiles: {len(empty_profiles)}\")\n",
        "\n",
        "        if quality_profiles:\n",
        "            print(f\"\\n‚úÖ HIGH-QUALITY PROFILES ({len(quality_profiles)} profiles)\")\n",
        "            print(\"=\" * 65)\n",
        "            profiles_to_show = quality_profiles\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è  NO HIGH-QUALITY PROFILES FOUND - SHOWING ALL RESULTS\")\n",
        "            print(\"=\" * 65)\n",
        "            profiles_to_show = profiles\n",
        "    else:\n",
        "        print(f\"\\nüìã DISCOVERED LINKEDIN PROFILES ({len(profiles)} total)\")\n",
        "        print(\"=\" * 65)\n",
        "        profiles_to_show = profiles\n",
        "\n",
        "    for i, profile in enumerate(profiles_to_show[:max_display], 1):\n",
        "        quality_score = profile.get('_quality_score', 0)\n",
        "        print(f\"\\nüë§ PROFILE {i} (Quality Score: {quality_score}/10):\")\n",
        "        print(f\"   Name: {profile.get('name', 'N/A')}\")\n",
        "        print(f\"   LinkedIn ID: {profile.get('linkedin_id', 'N/A')}\")\n",
        "        print(f\"   Location: {profile.get('city', 'N/A')} {profile.get('state', '')} {profile.get('country', '')}\")\n",
        "        # Highlight empty or low-quality fields\n",
        "        company = profile.get('current_company_name', 'N/A')\n",
        "        position = profile.get('position', 'N/A')\n",
        "\n",
        "        if company in ['N/A', '--', None, '']:\n",
        "            print(f\"   Current Company: ‚ùå {company} (EMPTY)\")\n",
        "        else:\n",
        "            print(f\"   Current Company: ‚úÖ {company}\")\n",
        "\n",
        "        if position in ['N/A', '--', None, '']:\n",
        "            print(f\"   Position: ‚ùå {position} (EMPTY)\")\n",
        "        else:\n",
        "            print(f\"   Position: ‚úÖ {position}\")\n",
        "\n",
        "        # Industry with validation\n",
        "        industry = profile.get('industry', 'N/A')\n",
        "        if industry in ['N/A', '--', None, '']:\n",
        "            print(f\"   Industry: ‚ùå {industry} (EMPTY)\")\n",
        "        else:\n",
        "            print(f\"   Industry: ‚úÖ {industry}\")\n",
        "\n",
        "        # Followers/Connections with validation\n",
        "        followers = profile.get('followers', 'N/A')\n",
        "        connections = profile.get('connections', 'N/A')\n",
        "\n",
        "        if str(followers).isdigit():\n",
        "            print(f\"   Followers: ‚úÖ {followers}\")\n",
        "        else:\n",
        "            print(f\"   Followers: ‚ùå {followers} (EMPTY)\")\n",
        "\n",
        "        if str(connections).isdigit():\n",
        "            print(f\"   Connections: ‚úÖ {connections}\")\n",
        "        else:\n",
        "            print(f\"   Connections: ‚ùå {connections} (EMPTY)\")\n",
        "\n",
        "        # About section (truncated)\n",
        "        about = profile.get('about', '')\n",
        "        if about:\n",
        "            about_preview = about[:150] + \"...\" if len(about) > 150 else about\n",
        "            print(f\"   About: {about_preview}\")\n",
        "\n",
        "        # Education\n",
        "        education = profile.get('education', [])\n",
        "        if education and len(education) > 0:\n",
        "            edu = education[0]\n",
        "            school = edu.get('title', edu.get('school', 'N/A'))\n",
        "            degree = edu.get('degree', '')\n",
        "            if degree:\n",
        "                print(f\"   Education: {degree} at {school}\")\n",
        "            else:\n",
        "                print(f\"   Education: {school}\")\n",
        "\n",
        "        # Experience\n",
        "        experience = profile.get('experience', [])\n",
        "        if experience and len(experience) > 0:\n",
        "            exp = experience[0]\n",
        "            title = exp.get('title', 'N/A')\n",
        "            company = exp.get('company', 'N/A')\n",
        "            print(f\"   Latest Experience: {title} at {company}\")\n",
        "\n",
        "        # Profile URL\n",
        "        url = profile.get('url', profile.get('linkedin_url', 'N/A'))\n",
        "        print(f\"   Profile URL: {url}\")\n",
        "\n",
        "        # Match confidence (if available)\n",
        "        if 'match_score' in profile or 'confidence' in profile:\n",
        "            score = profile.get('match_score', profile.get('confidence', 'N/A'))\n",
        "            print(f\"   Match Confidence: {score}\")\n",
        "\n",
        "    if len(profiles_to_show) > max_display:\n",
        "        print(f\"\\n... and {len(profiles_to_show) - max_display} more quality profiles\")\n",
        "\n",
        "    # Show summary of filtered out profiles\n",
        "    if show_quality_filter and len(empty_profiles) > 0:\n",
        "        print(f\"\\n‚ùå FILTERED OUT {len(empty_profiles)} LOW-QUALITY PROFILES:\")\n",
        "        for i, profile in enumerate(empty_profiles[:3], 1):\n",
        "            name = profile.get('name', 'Unknown')\n",
        "            linkedin_id = profile.get('linkedin_id', 'N/A')\n",
        "            quality_score = profile.get('_quality_score', 0)\n",
        "            print(f\"   {i}. {name} (ID: {linkedin_id}, Quality: {quality_score}/10)\")\n",
        "\n",
        "        if len(empty_profiles) > 3:\n",
        "            print(f\"   ... and {len(empty_profiles) - 3} more low-quality profiles\")\n",
        "\n",
        "\n",
        "def save_discovery_results(results: List[Dict], search_info: Dict = None) -> str:\n",
        "    \"\"\"Save discovery results to JSON file with metadata\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"linkedin_name_discovery_{timestamp}.json\"\n",
        "\n",
        "    # Analyze results for metadata\n",
        "    companies = [p.get('current_company_name') for p in results if p.get('current_company_name')]\n",
        "    locations = [p.get('city') for p in results if p.get('city')]\n",
        "    industries = [p.get('industry') for p in results if p.get('industry')]\n",
        "\n",
        "    metadata = {\n",
        "        \"discovery_timestamp\": datetime.now().isoformat(),\n",
        "        \"total_profiles_found\": len(results),\n",
        "        \"unique_companies\": len(set(companies)) if companies else 0,\n",
        "        \"unique_locations\": len(set(locations)) if locations else 0,\n",
        "        \"unique_industries\": len(set(industries)) if industries else 0,\n",
        "        \"scraper_type\": \"bright_data_name_discovery\"\n",
        "    }\n",
        "\n",
        "    if search_info:\n",
        "        metadata[\"search_parameters\"] = search_info\n",
        "\n",
        "    data_package = {\n",
        "        \"metadata\": metadata,\n",
        "        \"discovered_profiles\": results\n",
        "    }\n",
        "\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data_package, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Discovery results saved to: {filename}\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function demonstrating LinkedIn name discovery\"\"\"\n",
        "\n",
        "    # Configuration with your API token\n",
        "    API_TOKEN = \"\"\n",
        "    DATASET_ID = \"gd_l1viktl72bvl7bjuj0\"\n",
        "\n",
        "    print(\"üîç BRIGHT DATA LINKEDIN NAME DISCOVERY SCRAPER\")\n",
        "    print(\"Find LinkedIn profiles by searching names\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    # Example: Search for people by name\n",
        "    people_to_discover = [\n",
        "        {\"first_name\": \"Shashikant\", \"last_name\": \"Zarekar\"}\n",
        "    ]\n",
        "\n",
        "    # Optional: Add more specific search parameters to get better results\n",
        "    additional_search_params = {\n",
        "        \"company\": \"Tech\",  # Uncomment and modify to filter by company keywords\n",
        "        \"location\": \"India\",  # Uncomment to filter by location\n",
        "        # \"industry\": \"Technology\"  # Uncomment to filter by industry\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüéØ SEARCH CONFIGURATION:\")\n",
        "    print(f\"   People to find: {len(people_to_discover)}\")\n",
        "    if additional_search_params:\n",
        "        print(f\"   Additional filters: {additional_search_params}\")\n",
        "    else:\n",
        "        print(f\"   Additional filters: None (will find all matches)\")\n",
        "\n",
        "    # Run the discovery\n",
        "    results = discover_linkedin_profiles_by_names(\n",
        "        API_TOKEN,\n",
        "        DATASET_ID,\n",
        "        people_to_discover,\n",
        "        additional_search_params\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        # Display discovered profiles\n",
        "        display_discovered_profiles(results)\n",
        "\n",
        "        # Save results\n",
        "        search_info = {\n",
        "            \"people_searched\": people_to_discover,\n",
        "            \"additional_params\": additional_search_params\n",
        "        }\n",
        "        saved_file = save_discovery_results(results, search_info)\n",
        "\n",
        "        # Show discovery statistics\n",
        "        print(f\"\\nüìä DISCOVERY STATISTICS:\")\n",
        "        print(f\"   Total profiles found: {len(results)}\")\n",
        "\n",
        "        # Analyze companies\n",
        "        companies = [p.get('current_company_name') for p in results if p.get('current_company_name')]\n",
        "        if companies:\n",
        "            from collections import Counter\n",
        "            top_companies = Counter(companies).most_common(5)\n",
        "            print(f\"   Top companies found: {[comp for comp, count in top_companies]}\")\n",
        "\n",
        "        # Analyze locations\n",
        "        locations = [p.get('city') for p in results if p.get('city')]\n",
        "        if locations:\n",
        "            unique_locations = len(set(locations))\n",
        "            print(f\"   Locations represented: {unique_locations}\")\n",
        "\n",
        "        # Analyze industries\n",
        "        industries = [p.get('industry') for p in results if p.get('industry')]\n",
        "        if industries:\n",
        "            unique_industries = len(set(industries))\n",
        "            print(f\"   Industries represented: {unique_industries}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Discovery completed successfully!\")\n",
        "        print(f\"   Results saved to: {saved_file}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n‚ùå No profiles discovered\")\n",
        "        print(f\"\\nüîß TROUBLESHOOTING TIPS:\")\n",
        "        print(f\"   1. Try more common names\")\n",
        "        print(f\"   2. Remove location/company filters if used\")\n",
        "        print(f\"   3. Check that your dataset supports name discovery\")\n",
        "        print(f\"   4. Wait longer - discovery jobs can take 10+ minutes\")\n",
        "\n",
        "        # Test API connectivity\n",
        "        scraper = BrightDataLinkedInNameScraper(API_TOKEN, DATASET_ID)\n",
        "        print(f\"\\nüß™ Testing API connectivity...\")\n",
        "        snapshots = scraper.get_snapshots(\"ready\")\n",
        "        if snapshots:\n",
        "            snapshot_count = len(snapshots.get('data', [])) if isinstance(snapshots, dict) else len(snapshots)\n",
        "            print(f\"‚úÖ API working - found {snapshot_count} ready snapshots\")\n",
        "        else:\n",
        "            print(f\"‚ùå API connection issues\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUuKD4qe7Gxu",
        "outputId": "56d9b07c-d79c-47eb-a14c-9b1748ca9a77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç BRIGHT DATA LINKEDIN NAME DISCOVERY SCRAPER\n",
            "Find LinkedIn profiles by searching names\n",
            "=================================================================\n",
            "\n",
            "üéØ SEARCH CONFIGURATION:\n",
            "   People to find: 1\n",
            "   Additional filters: {'company': 'Tech', 'location': 'India'}\n",
            "üîç BRIGHT DATA LINKEDIN NAME DISCOVERY\n",
            "Using OFFICIAL API endpoints for name-based search\n",
            "=======================================================\n",
            "üîç Triggering LinkedIn name discovery for 1 people...\n",
            "   1. Shashikant Zarekar\n",
            "API URL: https://api.brightdata.com/datasets/v3/trigger\n",
            "Dataset ID: gd_l1viktl72bvl7bjuj0\n",
            "Search parameters: {'dataset_id': 'gd_l1viktl72bvl7bjuj0', 'include_errors': 'true', 'type': 'discover_new', 'discover_by': 'name', 'company': 'Tech', 'location': 'India'}\n",
            "Response status: 200\n",
            "‚úÖ Name discovery job triggered successfully!\n",
            "Snapshot ID: sd_mfxtcd8g2mo9wi9shd\n",
            "üéØ Discovery job started with snapshot ID: sd_mfxtcd8g2mo9wi9shd\n",
            "\n",
            "‚è≥ WAITING FOR DISCOVERY COMPLETION...\n",
            "‚è≥ Waiting for discovery job sd_mfxtcd8g2mo9wi9shd to complete...\n",
            "üí° Discovery jobs may take longer - timeout after 600 seconds\n",
            "Attempt 1: Status 202 (0s elapsed)\n",
            "‚è≥ Job still processing... (discovery can take several minutes)\n",
            "Attempt 2: Status 200 (20s elapsed)\n",
            "‚úÖ Discovery job sd_mfxtcd8g2mo9wi9shd is ready!\n",
            "\n",
            "üì• DOWNLOADING RESULTS...\n",
            "üì° Downloading discovery results from snapshot: sd_mfxtcd8g2mo9wi9shd\n",
            "‚úÖ Successfully downloaded discovery data!\n",
            "‚úÖ Successfully discovered 5 profiles!\n",
            "\n",
            "üìã LINKEDIN PROFILE ANALYSIS\n",
            "=================================================================\n",
            "   Total profiles found: 5\n",
            "   High-quality profiles: 1\n",
            "   Low-quality/empty profiles: 4\n",
            "\n",
            "‚úÖ HIGH-QUALITY PROFILES (1 profiles)\n",
            "=================================================================\n",
            "\n",
            "üë§ PROFILE 1 (Quality Score: 10/10):\n",
            "   Name: Shashikant Zarekar\n",
            "   LinkedIn ID: shashikant-zarekar-62594222\n",
            "   Location: Mumbai, Maharashtra, India  \n",
            "   Current Company: ‚úÖ Gem Engserv Pvt. Ltd\n",
            "   Position: ‚úÖ Senior Brand Manager | PGDM Marketing | BTech VJTI | Ex. L&T , Tata Realty\n",
            "   Industry: ‚ùå N/A (EMPTY)\n",
            "   Followers: ‚úÖ 9611\n",
            "   Connections: ‚úÖ 500\n",
            "   About: Shashikant has a strong passion towards marketing and would like to become a leading marketing professional in the industry. Core Expertise in Strateg...\n",
            "   Education: Master of Business Administration (M.B.A.) at ITM Group of Institutions\n",
            "   Latest Experience: Gem Engserv Pvt. Ltd at Gem Engserv Pvt. Ltd\n",
            "   Profile URL: https://at.linkedin.com/in/shashikant-zarekar-62594222\n",
            "\n",
            "‚ùå FILTERED OUT 4 LOW-QUALITY PROFILES:\n",
            "   1. shashikant zarekar (ID: shashikant-zarekar-aa6481336, Quality: 1/10)\n",
            "   2. shashikant zarekar (ID: shashikant-zarekar-46a6aa23, Quality: 2/10)\n",
            "   3. Shashikant zarekar (ID: shashikant-zarekar-096648381, Quality: 1/10)\n",
            "   ... and 1 more low-quality profiles\n",
            "\n",
            "üíæ Discovery results saved to: linkedin_name_discovery_20250924_100035.json\n",
            "\n",
            "üìä DISCOVERY STATISTICS:\n",
            "   Total profiles found: 5\n",
            "   Top companies found: ['Gem Engserv Pvt. Ltd']\n",
            "   Locations represented: 2\n",
            "\n",
            "‚úÖ Discovery completed successfully!\n",
            "   Results saved to: linkedin_name_discovery_20250924_100035.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}