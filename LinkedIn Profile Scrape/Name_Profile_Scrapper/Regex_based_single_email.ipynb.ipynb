{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUuKD4qe7Gxu",
        "outputId": "736dc90f-cb3d-4877-9b3d-0fd99661647a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° ENHANCED LINKEDIN DISCOVERY WITH QUALITY FILTERING\n",
            "============================================================\n",
            "Target: Purva Ingle\n",
            "Company pattern: '.*Gem.*'\n",
            "‚ö° SMART LINKEDIN DISCOVERY WITH QUALITY FILTERING\n",
            "============================================================\n",
            "Company filter: '.*Gem.*'\n",
            "Quality threshold: 4/10\n",
            "Max wait time: 300s\n",
            "üîç Triggering LinkedIn name discovery for 1 people...\n",
            "   1. Purva Ingle\n",
            "API URL: https://api.brightdata.com/datasets/v3/trigger\n",
            "Dataset ID: gd_l1viktl72bvl7bjuj0\n",
            "Search parameters: {'dataset_id': 'gd_l1viktl72bvl7bjuj0', 'include_errors': 'true', 'type': 'discover_new', 'discover_by': 'name', 'location': 'India'}\n",
            "Response status: 200\n",
            "‚úÖ Name discovery job triggered successfully!\n",
            "Snapshot ID: s_mfxx2dog2oe6xkz206\n",
            "üöÄ Job started: s_mfxx2dog2oe6xkz206\n",
            "‚ö° Smart waiting with early termination enabled\n",
            "   Target company pattern: '.*Gem.*'\n",
            "   Minimum quality score for early termination: 4/10\n",
            "   Early check interval: 5s\n",
            "üîç Early check 1 (0s) - Looking for quick matches...\n",
            "üîç Early check 2 (7s) - Looking for quick matches...\n",
            "üîç Early check 3 (14s) - Looking for quick matches...\n",
            "üîç Early check 4 (21s) - Looking for quick matches...\n",
            "üîç Early check 5 (27s) - Looking for quick matches...\n",
            "üîç Early check 6 (34s) - Looking for quick matches...\n",
            "üîç Early check 7 (40s) - Looking for quick matches...\n",
            "üîç Early check 8 (46s) - Looking for quick matches...\n",
            "üîç Early check 9 (52s) - Looking for quick matches...\n",
            "üîç Early check 10 (59s) - Looking for quick matches...\n",
            "‚è≥ No early matches found, continuing with normal polling...\n",
            "Attempt 11: Status 202 (66s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 12: Status 202 (83s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 13: Status 202 (100s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 14: Status 202 (118s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 15: Status 202 (135s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 16: Status 202 (152s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 17: Status 202 (168s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 18: Status 202 (185s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 19: Status 202 (202s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 20: Status 202 (218s elapsed)\n",
            "‚è≥ Job still processing...\n",
            "Attempt 21: Status 200 (234s elapsed)\n",
            "‚úÖ Discovery job s_mfxx2dog2oe6xkz206 completed!\n",
            "\n",
            "üìã LINKEDIN PROFILE ANALYSIS\n",
            "==================================================\n",
            "Total profiles found: 1\n",
            "High-quality profiles: 1\n",
            "Low-quality/skeleton profiles: 0\n",
            "\n",
            "‚úÖ HIGH-QUALITY PROFILES (1 profiles)\n",
            "----------------------------------------\n",
            "\n",
            "üë§ PROFILE 1 (Quality Score: 8/10):\n",
            "   Name: Purva Ingle\n",
            "   Current Company: ‚úÖ Gem Engserv Pvt. Ltd\n",
            "   Position: ‚úÖ Assistant Manager-Business and Operations\n",
            "   About Section: ‚ùå 0 characters\n",
            "   Experience: ‚úÖ 6 entries\n",
            "   Network: ‚úÖ 169 connections/followers\n",
            "   Profile URL: https://sa.linkedin.com/in/Purva-ingle-4a0b75b5\n",
            "\n",
            "üíæ High-quality results saved to: linkedin_quality_results_20250924_114822.json\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "class BrightDataLinkedInNameScraper:\n",
        "    def __init__(self, api_token: str, dataset_id: str = \"gd_l1viktl72bvl7bjuj0\"):\n",
        "        \"\"\"\n",
        "        Initialize LinkedIn name-based scraper with Bright Data API\n",
        "\n",
        "        Args:\n",
        "            api_token: Your Bright Data API token\n",
        "            dataset_id: Your LinkedIn scraper dataset ID\n",
        "        \"\"\"\n",
        "        self.api_token = api_token\n",
        "        self.dataset_id = dataset_id\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        self.base_url = \"https://api.brightdata.com/datasets/v3\"\n",
        "\n",
        "    def trigger_name_discovery(self, people: List[Dict[str, str]],\n",
        "                             additional_params: Optional[Dict] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Trigger LinkedIn profile discovery using names\n",
        "\n",
        "        Args:\n",
        "            people: List of dictionaries with 'first_name' and 'last_name'\n",
        "            additional_params: Optional additional search parameters (company, location, etc.)\n",
        "\n",
        "        Returns:\n",
        "            API response with job details including snapshot_id\n",
        "        \"\"\"\n",
        "        # Validate input data\n",
        "        for person in people:\n",
        "            if 'first_name' not in person or 'last_name' not in person:\n",
        "                return {\"error\": \"Each person must have 'first_name' and 'last_name'\"}\n",
        "\n",
        "        api_url = f\"{self.base_url}/trigger\"\n",
        "        params = {\n",
        "            \"dataset_id\": self.dataset_id,\n",
        "            \"include_errors\": \"true\",\n",
        "            \"type\": \"discover_new\",\n",
        "            \"discover_by\": \"name\"\n",
        "        }\n",
        "\n",
        "        # Add any additional search parameters\n",
        "        if additional_params:\n",
        "            params.update(additional_params)\n",
        "\n",
        "        print(f\"üîç Triggering LinkedIn name discovery for {len(people)} people...\")\n",
        "        for i, person in enumerate(people, 1):\n",
        "            name_display = f\"{person['first_name']} {person['last_name']}\"\n",
        "            # Add company/location if provided in person data\n",
        "            if 'company' in person:\n",
        "                name_display += f\" (Company: {person['company']})\"\n",
        "            if 'location' in person:\n",
        "                name_display += f\" (Location: {person['location']})\"\n",
        "            print(f\"   {i}. {name_display}\")\n",
        "\n",
        "        print(f\"API URL: {api_url}\")\n",
        "        print(f\"Dataset ID: {self.dataset_id}\")\n",
        "        print(f\"Search parameters: {params}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=self.headers, json=people, params=params)\n",
        "\n",
        "            print(f\"Response status: {response.status_code}\")\n",
        "\n",
        "            if response.status_code in [200, 201, 202]:\n",
        "                result = response.json()\n",
        "                print(f\"‚úÖ Name discovery job triggered successfully!\")\n",
        "                print(f\"Snapshot ID: {result.get('snapshot_id')}\")\n",
        "                return result\n",
        "            else:\n",
        "                print(f\"‚ùå Request failed: {response.status_code}\")\n",
        "                print(f\"Response: {response.text}\")\n",
        "                return {\"error\": f\"HTTP {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error triggering discovery: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def check_partial_results(self, snapshot_id: str) -> Tuple[Optional[List[Dict]], bool]:\n",
        "        \"\"\"\n",
        "        Check for partial results from an ongoing job\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (partial_data, job_complete)\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/snapshot/{snapshot_id}\"\n",
        "        params = {\"format\": \"json\"}\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "\n",
        "                # Handle different response formats\n",
        "                if isinstance(data, list):\n",
        "                    return data, True\n",
        "                elif isinstance(data, dict):\n",
        "                    if 'data' in data:\n",
        "                        return data['data'], True\n",
        "                    elif 'results' in data:\n",
        "                        return data['results'], True\n",
        "                    # Check if there are partial results available\n",
        "                    elif 'partial_data' in data:\n",
        "                        return data['partial_data'], False\n",
        "                    else:\n",
        "                        return [data], True\n",
        "\n",
        "                return [], True\n",
        "\n",
        "            elif response.status_code == 202:\n",
        "                # Job still running, check if any partial data is available\n",
        "                try:\n",
        "                    response_data = response.json()\n",
        "                    if 'partial_results' in response_data:\n",
        "                        return response_data['partial_results'], False\n",
        "                    elif 'current_results' in response_data:\n",
        "                        return response_data['current_results'], False\n",
        "                except:\n",
        "                    pass\n",
        "                return None, False\n",
        "\n",
        "            else:\n",
        "                return None, False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error checking partial results: {e}\")\n",
        "            return None, False\n",
        "\n",
        "    def calculate_quality_score(self, profile: Dict) -> int:\n",
        "        \"\"\"\n",
        "        Enhanced quality scoring system for profile completeness (1-10 scale)\n",
        "\n",
        "        Focuses on actual profile data completeness rather than just connections\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        max_score = 10\n",
        "\n",
        "        # 1. Complete name (1 point)\n",
        "        name = profile.get('name', '').strip()\n",
        "        if name and len(name) > 5 and ' ' in name:  # Has both first and last name\n",
        "            score += 1\n",
        "\n",
        "        # 2. Current company information (2 points)\n",
        "        current_company = profile.get('current_company', {})\n",
        "        current_company_name = ''\n",
        "\n",
        "        if isinstance(current_company, dict):\n",
        "            current_company_name = current_company.get('name', '').strip()\n",
        "        else:\n",
        "            current_company_name = str(current_company).strip() if current_company else ''\n",
        "\n",
        "        if not current_company_name:\n",
        "            current_company_name = profile.get('current_company_name', '').strip()\n",
        "\n",
        "        if current_company_name and current_company_name.lower() not in ['n/a', 'unknown', '-', '']:\n",
        "            score += 2\n",
        "\n",
        "        # 3. Job position/title (2 points)\n",
        "        position = ''\n",
        "        if isinstance(current_company, dict):\n",
        "            position = current_company.get('title', '').strip()\n",
        "\n",
        "        if not position:\n",
        "            position = profile.get('position', '').strip()\n",
        "        if not position:\n",
        "            position = profile.get('current_position', '').strip()\n",
        "        if not position:\n",
        "            position = profile.get('headline', '').strip()\n",
        "\n",
        "        if position and position.lower() not in ['n/a', 'unknown', '-', '']:\n",
        "            score += 2\n",
        "\n",
        "        # 4. About section (2 points if substantial)\n",
        "        about = profile.get('about', '').strip()\n",
        "        if about and len(about) > 50:  # Substantial about section\n",
        "            score += 2\n",
        "        elif about and len(about) > 10:  # Minimal about section\n",
        "            score += 1\n",
        "\n",
        "        # 5. Experience history (1 point)\n",
        "        experience = profile.get('experience', [])\n",
        "        if isinstance(experience, list) and len(experience) > 0:\n",
        "            # Check if experience has actual data\n",
        "            has_real_experience = any(\n",
        "                exp.get('company', '').strip() and exp.get('title', '').strip()\n",
        "                for exp in experience if isinstance(exp, dict)\n",
        "            )\n",
        "            if has_real_experience:\n",
        "                score += 1\n",
        "\n",
        "        # 6. Education information (1 point)\n",
        "        education = profile.get('education', [])\n",
        "        if isinstance(education, list) and len(education) > 0:\n",
        "            # Check if education has actual data\n",
        "            has_real_education = any(\n",
        "                edu.get('school', '').strip() or edu.get('degree', '').strip()\n",
        "                for edu in education if isinstance(edu, dict)\n",
        "            )\n",
        "            if has_real_education:\n",
        "                score += 1\n",
        "\n",
        "        # 7. Follower/connection count (1 point - indicates active profile)\n",
        "        followers = profile.get('followers', 0)\n",
        "        connections = profile.get('connections', 0)\n",
        "\n",
        "        # Handle different formats for connections/followers\n",
        "        try:\n",
        "            if isinstance(followers, str):\n",
        "                followers = int(followers.replace(',', '').replace('+', ''))\n",
        "            if isinstance(connections, str):\n",
        "                connections = int(connections.replace(',', '').replace('+', ''))\n",
        "\n",
        "            if followers > 50 or connections > 50:  # Has some network presence\n",
        "                score += 1\n",
        "        except (ValueError, AttributeError):\n",
        "            pass\n",
        "\n",
        "        # Bonus: Profile URL exists and looks complete\n",
        "        url = profile.get('url', '')\n",
        "        if url and 'linkedin.com/in/' in url and len(url) > 30:\n",
        "            # This is counted within the max score, not as bonus\n",
        "            pass\n",
        "\n",
        "        return min(score, max_score)  # Cap at maximum score\n",
        "\n",
        "    def filter_quality_profiles(self, profiles: List[Dict], min_quality_score: int = 3) -> Tuple[List[Dict], List[Dict]]:\n",
        "        \"\"\"\n",
        "        Filter profiles based on quality score and return both high and low quality lists\n",
        "\n",
        "        Args:\n",
        "            profiles: List of discovered profiles\n",
        "            min_quality_score: Minimum score to be considered high-quality (1-10 scale)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (high_quality_profiles, low_quality_profiles)\n",
        "        \"\"\"\n",
        "        if not profiles:\n",
        "            return [], []\n",
        "\n",
        "        high_quality = []\n",
        "        low_quality = []\n",
        "\n",
        "        for profile in profiles:\n",
        "            score = self.calculate_quality_score(profile)\n",
        "            profile['_quality_score'] = score\n",
        "\n",
        "            if score >= min_quality_score:\n",
        "                high_quality.append(profile)\n",
        "            else:\n",
        "                low_quality.append(profile)\n",
        "\n",
        "        # Sort high quality profiles by score (highest first)\n",
        "        high_quality.sort(key=lambda x: x['_quality_score'], reverse=True)\n",
        "\n",
        "        return high_quality, low_quality\n",
        "\n",
        "    def wait_with_early_termination(self,\n",
        "                                  snapshot_id: str,\n",
        "                                  company_pattern: str,\n",
        "                                  case_sensitive: bool = False,\n",
        "                                  min_quality_score: int = 4,  # Increased default threshold\n",
        "                                  max_wait: int = 600,\n",
        "                                  check_interval: int = 15,\n",
        "                                  early_check_interval: int = 5) -> Optional[List[Dict]]:\n",
        "        \"\"\"\n",
        "        Wait for job completion with early termination when high-quality matches are found\n",
        "\n",
        "        Args:\n",
        "            snapshot_id: The snapshot ID to wait for\n",
        "            company_pattern: Regex pattern for company filtering\n",
        "            case_sensitive: Whether regex should be case sensitive\n",
        "            min_quality_score: Minimum score to trigger early termination (1-10 scale)\n",
        "            max_wait: Maximum wait time in seconds\n",
        "            check_interval: Normal check interval\n",
        "            early_check_interval: Frequent check interval for early results\n",
        "\n",
        "        Returns:\n",
        "            List of matching profiles or None\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        print(f\"‚ö° Smart waiting with early termination enabled\")\n",
        "        print(f\"   Target company pattern: '{company_pattern}'\")\n",
        "        print(f\"   Minimum quality score for early termination: {min_quality_score}/10\")\n",
        "        print(f\"   Early check interval: {early_check_interval}s\")\n",
        "\n",
        "        # Compile regex pattern\n",
        "        flags = 0 if case_sensitive else re.IGNORECASE\n",
        "        try:\n",
        "            pattern = re.compile(company_pattern, flags)\n",
        "        except re.error as e:\n",
        "            print(f\"‚ùå Invalid regex pattern '{company_pattern}': {e}\")\n",
        "            return None\n",
        "\n",
        "        attempts = 0\n",
        "        best_matches = []\n",
        "\n",
        "        # Phase 1: Frequent early checks (first 60 seconds)\n",
        "        phase1_duration = 60\n",
        "        while time.time() - start_time < min(phase1_duration, max_wait):\n",
        "            attempts += 1\n",
        "            elapsed = int(time.time() - start_time)\n",
        "\n",
        "            print(f\"üîç Early check {attempts} ({elapsed}s) - Looking for quick matches...\")\n",
        "\n",
        "            partial_data, job_complete = self.check_partial_results(snapshot_id)\n",
        "\n",
        "            if partial_data:\n",
        "                print(f\"üìä Found {len(partial_data)} profiles so far...\")\n",
        "\n",
        "                # Apply company filtering to partial results\n",
        "                filtered_profiles = self.filter_profiles_by_company_regex(\n",
        "                    partial_data, pattern, company_pattern\n",
        "                )\n",
        "\n",
        "                if filtered_profiles:\n",
        "                    # Apply quality filtering\n",
        "                    high_quality, low_quality = self.filter_quality_profiles(\n",
        "                        filtered_profiles, min_quality_score=min_quality_score\n",
        "                    )\n",
        "\n",
        "                    print(f\"üìã Quality Analysis: {len(high_quality)} high-quality, {len(low_quality)} low-quality\")\n",
        "\n",
        "                    if high_quality:\n",
        "                        best_match = high_quality[0]  # Already sorted by score\n",
        "                        print(f\"‚ö° HIGH QUALITY MATCH FOUND! Terminating early.\")\n",
        "                        print(f\"   Name: {best_match.get('name', 'Unknown')}\")\n",
        "                        print(f\"   Quality Score: {best_match['_quality_score']}/10\")\n",
        "                        print(f\"   Company: {best_match.get('current_company', {}).get('name', 'N/A') if isinstance(best_match.get('current_company'), dict) else best_match.get('current_company', 'N/A')}\")\n",
        "                        print(f\"   Time saved: ~{max_wait - elapsed} seconds\")\n",
        "                        return high_quality\n",
        "\n",
        "                    # Keep track of any matches, even if not high quality yet\n",
        "                    best_matches = filtered_profiles\n",
        "\n",
        "            if job_complete:\n",
        "                print(f\"‚úÖ Job completed during early phase!\")\n",
        "                if best_matches:\n",
        "                    # Apply final quality filtering\n",
        "                    high_quality, low_quality = self.filter_quality_profiles(best_matches)\n",
        "                    return high_quality if high_quality else best_matches\n",
        "                break\n",
        "\n",
        "            time.sleep(early_check_interval)\n",
        "\n",
        "        # Phase 2: Normal checking if no early termination\n",
        "        if best_matches:\n",
        "            print(f\"üìà Continuing with normal checks (found {len(best_matches)} matches)\")\n",
        "        else:\n",
        "            print(f\"‚è≥ No early matches found, continuing with normal polling...\")\n",
        "\n",
        "        while time.time() - start_time < max_wait:\n",
        "            attempts += 1\n",
        "            elapsed = int(time.time() - start_time)\n",
        "\n",
        "            url = f\"{self.base_url}/snapshot/{snapshot_id}\"\n",
        "            params = {\"format\": \"json\"}\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "                print(f\"Attempt {attempts}: Status {response.status_code} ({elapsed}s elapsed)\")\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    print(f\"‚úÖ Discovery job {snapshot_id} completed!\")\n",
        "                    data = response.json()\n",
        "\n",
        "                    # Handle different response formats\n",
        "                    if isinstance(data, list):\n",
        "                        final_data = data\n",
        "                    elif isinstance(data, dict):\n",
        "                        if 'data' in data:\n",
        "                            final_data = data['data']\n",
        "                        elif 'results' in data:\n",
        "                            final_data = data['results']\n",
        "                        else:\n",
        "                            final_data = [data]\n",
        "                    else:\n",
        "                        final_data = []\n",
        "\n",
        "                    if final_data:\n",
        "                        # Apply company filtering\n",
        "                        filtered_results = self.filter_profiles_by_company_regex(\n",
        "                            final_data, pattern, company_pattern\n",
        "                        )\n",
        "\n",
        "                        # Apply quality filtering\n",
        "                        if filtered_results:\n",
        "                            high_quality, low_quality = self.filter_quality_profiles(filtered_results)\n",
        "                            return high_quality if high_quality else filtered_results\n",
        "\n",
        "                        return filtered_results\n",
        "\n",
        "                elif response.status_code == 202:\n",
        "                    print(f\"‚è≥ Job still processing...\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Unexpected status: {response.status_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error checking job status: {e}\")\n",
        "\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        print(f\"‚è∞ Timeout reached, returning best matches found so far\")\n",
        "        if best_matches:\n",
        "            high_quality, low_quality = self.filter_quality_profiles(best_matches)\n",
        "            return high_quality if high_quality else best_matches\n",
        "        return None\n",
        "\n",
        "    def filter_profiles_by_company_regex(self, profiles: List[Dict], pattern, pattern_str: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Filter profiles using pre-compiled regex pattern\n",
        "        \"\"\"\n",
        "        if not profiles:\n",
        "            return []\n",
        "\n",
        "        matched_profiles = []\n",
        "\n",
        "        for profile in profiles:\n",
        "            profile_matched = False\n",
        "            match_details = []\n",
        "\n",
        "            # Check current company\n",
        "            current_company = profile.get('current_company', {})\n",
        "            if isinstance(current_company, dict):\n",
        "                current_company_name = current_company.get('name', '')\n",
        "            else:\n",
        "                current_company_name = str(current_company) if current_company else ''\n",
        "\n",
        "            if not current_company_name:\n",
        "                current_company_name = profile.get('current_company_name', '')\n",
        "\n",
        "            if current_company_name and pattern.search(current_company_name):\n",
        "                profile_matched = True\n",
        "                match_details.append(f\"Current: {current_company_name}\")\n",
        "\n",
        "            # Check experience companies\n",
        "            experience = profile.get('experience', [])\n",
        "            if isinstance(experience, list):\n",
        "                for exp in experience:\n",
        "                    if isinstance(exp, dict):\n",
        "                        exp_company = exp.get('company', '')\n",
        "                        if exp_company and pattern.search(exp_company):\n",
        "                            profile_matched = True\n",
        "                            match_details.append(f\"Experience: {exp_company}\")\n",
        "\n",
        "            if profile_matched:\n",
        "                profile['_company_matches'] = match_details\n",
        "                matched_profiles.append(profile)\n",
        "\n",
        "        return matched_profiles\n",
        "\n",
        "    def display_results_analysis(self, all_results: List[Dict], high_quality: List[Dict], low_quality: List[Dict]):\n",
        "        \"\"\"\n",
        "        Display detailed analysis of profile quality\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìã LINKEDIN PROFILE ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total profiles found: {len(all_results)}\")\n",
        "        print(f\"High-quality profiles: {len(high_quality)}\")\n",
        "        print(f\"Low-quality/skeleton profiles: {len(low_quality)}\")\n",
        "\n",
        "        if high_quality:\n",
        "            print(f\"\\n‚úÖ HIGH-QUALITY PROFILES ({len(high_quality)} profiles)\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            for i, profile in enumerate(high_quality, 1):\n",
        "                print(f\"\\nüë§ PROFILE {i} (Quality Score: {profile['_quality_score']}/10):\")\n",
        "                print(f\"   Name: {profile.get('name', 'N/A')}\")\n",
        "\n",
        "                # Current company\n",
        "                current_company = profile.get('current_company', {})\n",
        "                if isinstance(current_company, dict):\n",
        "                    company_name = current_company.get('name', 'N/A')\n",
        "                    position = current_company.get('title', 'N/A')\n",
        "                else:\n",
        "                    company_name = str(current_company) if current_company else 'N/A'\n",
        "                    position = profile.get('position', 'N/A')\n",
        "\n",
        "                print(f\"   Current Company: {'‚úÖ' if company_name != 'N/A' else '‚ùå'} {company_name}\")\n",
        "                print(f\"   Position: {'‚úÖ' if position != 'N/A' else '‚ùå'} {position}\")\n",
        "\n",
        "                # Additional info\n",
        "                about_len = len(profile.get('about', ''))\n",
        "                print(f\"   About Section: {'‚úÖ' if about_len > 50 else '‚ùå'} {about_len} characters\")\n",
        "\n",
        "                experience_count = len(profile.get('experience', []))\n",
        "                print(f\"   Experience: {'‚úÖ' if experience_count > 0 else '‚ùå'} {experience_count} entries\")\n",
        "\n",
        "                followers = profile.get('followers', 0) or profile.get('connections', 0)\n",
        "                print(f\"   Network: {'‚úÖ' if followers else '‚ùå'} {followers} connections/followers\")\n",
        "\n",
        "                print(f\"   Profile URL: {profile.get('url', 'N/A')}\")\n",
        "\n",
        "        if low_quality:\n",
        "            print(f\"\\n‚ùå FILTERED OUT {len(low_quality)} LOW-QUALITY PROFILES:\")\n",
        "            print(\"-\" * 40)\n",
        "            for i, profile in enumerate(low_quality, 1):\n",
        "                name = profile.get('name', 'Unknown')\n",
        "                profile_id = profile.get('url', 'N/A').split('/')[-1] if profile.get('url') else 'N/A'\n",
        "                score = profile.get('_quality_score', 0)\n",
        "                print(f\"{i}. {name} (ID: {profile_id}, Quality: {score}/10)\")\n",
        "\n",
        "\n",
        "def discover_linkedin_profiles_with_smart_termination(\n",
        "    api_token: str,\n",
        "    dataset_id: str,\n",
        "    people: List[Dict[str, str]],\n",
        "    company_regex_pattern: str,\n",
        "    additional_params: Optional[Dict] = None,\n",
        "    case_sensitive: bool = False,\n",
        "    min_quality_score: int = 4,  # Increased default to filter out skeleton profiles\n",
        "    max_wait: int = 600\n",
        ") -> Optional[List[Dict]]:\n",
        "    \"\"\"\n",
        "    Optimized LinkedIn profile discovery with smart early termination and quality filtering\n",
        "\n",
        "    Args:\n",
        "        api_token: Your Bright Data API token\n",
        "        dataset_id: Your dataset ID\n",
        "        people: List of dictionaries with 'first_name' and 'last_name'\n",
        "        company_regex_pattern: Regex pattern to match company names\n",
        "        additional_params: Optional global search parameters\n",
        "        case_sensitive: Whether company name matching should be case sensitive\n",
        "        min_quality_score: Score threshold for early termination (1-10 scale)\n",
        "        max_wait: Maximum wait time in seconds\n",
        "\n",
        "    Returns:\n",
        "        List of filtered profile data or None if failed\n",
        "    \"\"\"\n",
        "    scraper = BrightDataLinkedInNameScraper(api_token, dataset_id)\n",
        "\n",
        "    print(\"‚ö° SMART LINKEDIN DISCOVERY WITH QUALITY FILTERING\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Company filter: '{company_regex_pattern}'\")\n",
        "    print(f\"Quality threshold: {min_quality_score}/10\")\n",
        "    print(f\"Max wait time: {max_wait}s\")\n",
        "\n",
        "    # Remove company filter from API params to get broader initial results\n",
        "    api_params = additional_params.copy() if additional_params else {}\n",
        "    if 'company' in api_params:\n",
        "        api_params.pop('company')\n",
        "\n",
        "    # Trigger discovery\n",
        "    trigger_result = scraper.trigger_name_discovery(people, api_params)\n",
        "\n",
        "    if trigger_result.get(\"error\"):\n",
        "        print(\"‚ùå Failed to trigger discovery job\")\n",
        "        return None\n",
        "\n",
        "    snapshot_id = trigger_result.get(\"snapshot_id\")\n",
        "    if not snapshot_id:\n",
        "        print(\"‚ùå No snapshot ID received\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üöÄ Job started: {snapshot_id}\")\n",
        "\n",
        "    # Smart waiting with early termination\n",
        "    results = scraper.wait_with_early_termination(\n",
        "        snapshot_id=snapshot_id,\n",
        "        company_pattern=company_regex_pattern,\n",
        "        case_sensitive=case_sensitive,\n",
        "        min_quality_score=min_quality_score,\n",
        "        max_wait=max_wait\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Enhanced main function with quality filtering\"\"\"\n",
        "\n",
        "    API_TOKEN = \"\"  # Replace with your token\n",
        "    DATASET_ID = \"gd_l1viktl72bvl7bjuj0\"\n",
        "\n",
        "    print(\"‚ö° ENHANCED LINKEDIN DISCOVERY WITH QUALITY FILTERING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    people_to_discover = [\n",
        "        {\"first_name\": \"Purva\", \"last_name\": \"Ingle\"}\n",
        "    ]\n",
        "\n",
        "    COMPANY_REGEX_PATTERN = \".*Gem.*\"\n",
        "\n",
        "    # Enhanced search parameters for better targeting\n",
        "    additional_search_params = {\n",
        "        \"location\": \"India\",  # More specific targeting\n",
        "        # \"company\": \"Tech\"   # Removed to allow regex filtering instead\n",
        "    }\n",
        "\n",
        "    print(f\"Target: {people_to_discover[0]['first_name']} {people_to_discover[0]['last_name']}\")\n",
        "    print(f\"Company pattern: '{COMPANY_REGEX_PATTERN}'\")\n",
        "\n",
        "    # Run enhanced discovery with quality filtering\n",
        "    results = discover_linkedin_profiles_with_smart_termination(\n",
        "        API_TOKEN,\n",
        "        DATASET_ID,\n",
        "        people_to_discover,\n",
        "        COMPANY_REGEX_PATTERN,\n",
        "        additional_search_params,\n",
        "        case_sensitive=False,\n",
        "        min_quality_score=4,  # Only return profiles with score 4+/10\n",
        "        max_wait=300\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        # Create scraper instance for analysis display\n",
        "        scraper = BrightDataLinkedInNameScraper(API_TOKEN, DATASET_ID)\n",
        "\n",
        "        # Analyze all results (including low quality for reporting)\n",
        "        high_quality, low_quality = scraper.filter_quality_profiles(results, min_quality_score=4)\n",
        "        all_profiles = high_quality + low_quality\n",
        "\n",
        "        # Display comprehensive analysis\n",
        "        scraper.display_results_analysis(all_profiles, high_quality, low_quality)\n",
        "\n",
        "        if high_quality:\n",
        "            # Save only high-quality results\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"linkedin_quality_results_{timestamp}.json\"\n",
        "\n",
        "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(high_quality, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"\\nüíæ High-quality results saved to: {filename}\")\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è No high-quality profiles found. Consider lowering quality threshold.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå No matching profiles found\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}