{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4jLdTjYG60i",
        "outputId": "588d07de-b9fc-41cf-e50b-adbc1cbab67a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.181.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.30.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain google-generativeai requests pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 1: Setup and Configuration\n",
        "# Install required packages - FINAL LIST\n",
        "!pip install langchain google-generativeai requests pydantic\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "# Pydantic for structured outputs\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Google Generative AI\n",
        "import google.generativeai as genai\n",
        "\n",
        "# API Configuration - Replace with your actual API keys\n",
        "SERPER_API_KEY = \"\"  # Your working Serper API key\n",
        "BRIGHTDATA_API_TOKEN = \"\"\n",
        "GOOGLE_API_KEY = \"\"  # Replace with your Google AI API key\n",
        "\n",
        "# Configure Google AI\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Pydantic Models for Structured Outputs\n",
        "class DomainOutput(BaseModel):\n",
        "    domain: str = Field(description=\"Extracted domain from email\")\n",
        "    original_email: str = Field(description=\"Original email address\")\n",
        "\n",
        "class SearchQueryOutput(BaseModel):\n",
        "    search_query: str = Field(description=\"Optimized search query for the domain\")\n",
        "    domain: str = Field(description=\"Domain being searched\")\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    title: str = Field(description=\"Title of the search result\")\n",
        "    url: str = Field(description=\"URL of the search result\")\n",
        "    snippet: str = Field(description=\"Description/snippet of the search result\")\n",
        "\n",
        "class SearchResultsOutput(BaseModel):\n",
        "    results: List[SearchResult] = Field(description=\"List of top 5 search results\")\n",
        "    query_used: str = Field(description=\"Search query that was used\")\n",
        "\n",
        "class URLSelectionOutput(BaseModel):\n",
        "    selected_url: str = Field(description=\"The best URL selected by Gemini\")\n",
        "    reasoning: str = Field(description=\"Reasoning for URL selection\")\n",
        "    confidence_score: float = Field(description=\"Confidence score (0-1)\")\n",
        "\n",
        "class ScrapedContentOutput(BaseModel):\n",
        "    url: str = Field(description=\"URL that was scraped\")\n",
        "    html_content: str = Field(description=\"Raw HTML content from the page\")\n",
        "    scrape_status: str = Field(description=\"Status of scraping operation\")\n",
        "\n",
        "class FinalSummaryOutput(BaseModel):\n",
        "    summary: str = Field(description=\"One-line summary of the website\")\n",
        "    url: str = Field(description=\"URL of the summarized website\")\n",
        "    domain: str = Field(description=\"Domain of the website\")\n",
        "    timestamp: str = Field(description=\"When the analysis was completed\")\n",
        "\n",
        "# Custom Gemini LLM Wrapper\n",
        "class GeminiLLM(LLM):\n",
        "    model_name: str = \"gemini-1.5-flash\"  # Using Gemini 1.5 Flash for better performance\n",
        "    temperature: float = 0.1\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        try:\n",
        "            model = genai.GenerativeModel(self.model_name)\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "gemini_llm = GeminiLLM(temperature=0.1)\n",
        "\n",
        "# Custom Output Parsers\n",
        "class DomainOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> DomainOutput:\n",
        "        try:\n",
        "            import json\n",
        "            parsed = json.loads(text)\n",
        "            return DomainOutput(**parsed)\n",
        "        except:\n",
        "            # Fallback parsing\n",
        "            domain_match = re.search(r'\"domain\":\\s*\"([^\"]+)\"', text)\n",
        "            email_match = re.search(r'\"original_email\":\\s*\"([^\"]+)\"', text)\n",
        "\n",
        "            domain = domain_match.group(1) if domain_match else \"unknown\"\n",
        "            email = email_match.group(1) if email_match else \"unknown\"\n",
        "\n",
        "            return DomainOutput(domain=domain, original_email=email)\n",
        "\n",
        "class SearchQueryOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> SearchQueryOutput:\n",
        "        try:\n",
        "            import json\n",
        "            parsed = json.loads(text)\n",
        "            return SearchQueryOutput(**parsed)\n",
        "        except:\n",
        "            # Fallback parsing\n",
        "            query_match = re.search(r'\"search_query\":\\s*\"([^\"]+)\"', text)\n",
        "            domain_match = re.search(r'\"domain\":\\s*\"([^\"]+)\"', text)\n",
        "\n",
        "            query = query_match.group(1) if query_match else text.strip()\n",
        "            domain = domain_match.group(1) if domain_match else \"unknown\"\n",
        "\n",
        "            return SearchQueryOutput(search_query=query, domain=domain)\n",
        "\n",
        "class URLSelectionOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> URLSelectionOutput:\n",
        "        try:\n",
        "            import json\n",
        "            parsed = json.loads(text)\n",
        "            return URLSelectionOutput(**parsed)\n",
        "        except:\n",
        "            # Fallback parsing\n",
        "            url_match = re.search(r'\"selected_url\":\\s*\"([^\"]+)\"', text)\n",
        "            reasoning_match = re.search(r'\"reasoning\":\\s*\"([^\"]+)\"', text)\n",
        "            confidence_match = re.search(r'\"confidence_score\":\\s*([0-9.]+)', text)\n",
        "\n",
        "            url = url_match.group(1) if url_match else \"unknown\"\n",
        "            reasoning = reasoning_match.group(1) if reasoning_match else \"No reasoning provided\"\n",
        "            confidence = float(confidence_match.group(1)) if confidence_match else 0.5\n",
        "\n",
        "            return URLSelectionOutput(\n",
        "                selected_url=url,\n",
        "                reasoning=reasoning,\n",
        "                confidence_score=confidence\n",
        "            )\n",
        "\n",
        "class FinalSummaryOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> FinalSummaryOutput:\n",
        "        try:\n",
        "            import json\n",
        "            parsed = json.loads(text)\n",
        "            return FinalSummaryOutput(**parsed)\n",
        "        except:\n",
        "            # Fallback parsing - just use the text as summary\n",
        "            return FinalSummaryOutput(\n",
        "                summary=text.strip(),\n",
        "                url=\"unknown\",\n",
        "                domain=\"unknown\",\n",
        "                timestamp=datetime.now().isoformat()\n",
        "            )\n",
        "\n",
        "print(\"✅ Setup complete! All imports and configurations loaded.\")\n",
        "print(\"🔧 Remember to replace API keys with your actual keys before running the chain.\")\n",
        "print(\"📦 Required packages: langchain, google-generativeai, requests, pydantic\")\n",
        "print(\"📋 Next: Run Cell 2 to create the individual chain components.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDdtucU_HCY0",
        "outputId": "c5a813f0-2374-44f9-f75e-c1bbbc732eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.181.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.30.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "✅ Setup complete! All imports and configurations loaded.\n",
            "🔧 Remember to replace API keys with your actual keys before running the chain.\n",
            "📦 Required packages: langchain, google-generativeai, requests, pydantic\n",
            "📋 Next: Run Cell 2 to create the individual chain components.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 2: Chain Components and API Functions\n",
        "\n",
        "import time  # Added for polling delays\n",
        "\n",
        "# Utility Functions\n",
        "def extract_domain_from_email(email: str) -> DomainOutput:\n",
        "    \"\"\"Extract domain from email address\"\"\"\n",
        "    try:\n",
        "        # Simple regex to extract domain\n",
        "        domain_match = re.search(r'@([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', email)\n",
        "        if domain_match:\n",
        "            domain = domain_match.group(1)\n",
        "            return DomainOutput(domain=domain, original_email=email)\n",
        "        else:\n",
        "            return DomainOutput(domain=\"invalid\", original_email=email)\n",
        "    except Exception as e:\n",
        "        return DomainOutput(domain=\"error\", original_email=email)\n",
        "\n",
        "def call_serper_api(query: str) -> SearchResultsOutput:\n",
        "    \"\"\"Call Serper API to get search results - Updated to match working pattern\"\"\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"num\": 5  # Request 5 results\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"🔍 Searching Serper API: '{query}'\")\n",
        "        response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            organic_results = data.get('organic', [])[:5]\n",
        "\n",
        "            print(f\"✅ Found {len(organic_results)} results\")\n",
        "\n",
        "            results = []\n",
        "            for result in organic_results:\n",
        "                results.append(SearchResult(\n",
        "                    title=result.get('title', ''),\n",
        "                    url=result.get('link', ''),\n",
        "                    snippet=result.get('snippet', '')\n",
        "                ))\n",
        "\n",
        "            return SearchResultsOutput(results=results, query_used=query)\n",
        "\n",
        "        else:\n",
        "            print(f\"❌ Serper API Error: {response.status_code}\")\n",
        "            if response.status_code == 401:\n",
        "                print(\"💡 Check your API key is correct\")\n",
        "            elif response.status_code == 429:\n",
        "                print(\"💡 Rate limit exceeded - wait a moment\")\n",
        "\n",
        "            return SearchResultsOutput(results=[], query_used=query)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Serper API Exception: {e}\")\n",
        "        return SearchResultsOutput(results=[], query_used=query)\n",
        "\n",
        "def call_brightdata_api(url: str, dataset_id: str = \"gd_m6gjtfmeh43we6cqc\") -> ScrapedContentOutput:\n",
        "    \"\"\"\n",
        "    Call Bright Data API with proper two-phase workflow:\n",
        "    Phase 1: Trigger scraping job\n",
        "    Phase 2: Poll and retrieve results\n",
        "    \"\"\"\n",
        "    print(f\"\\n🤖 Starting Bright Data scraping for: {url}\")\n",
        "\n",
        "    # Phase 1: Trigger the scraping job\n",
        "    trigger_url = \"https://api.brightdata.com/datasets/v3/trigger\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {BRIGHTDATA_API_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        'dataset_id': dataset_id,\n",
        "        'format': 'json'\n",
        "    }\n",
        "\n",
        "    payload = [{\"url\": url}]\n",
        "\n",
        "    try:\n",
        "        print(\"⏳ Phase 1: Triggering scraping job...\")\n",
        "        response = requests.post(\n",
        "            trigger_url,\n",
        "            headers=headers,\n",
        "            params=params,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if not response.ok:\n",
        "            print(f\"❌ Trigger failed: {response.status_code} - {response.text}\")\n",
        "            return simple_scrape(url)\n",
        "\n",
        "        result = response.json()\n",
        "\n",
        "        if 'snapshot_id' not in result:\n",
        "            print(f\"❌ No snapshot_id in response: {result}\")\n",
        "            return simple_scrape(url)\n",
        "\n",
        "        snapshot_id = result['snapshot_id']\n",
        "        print(f\"✅ Job triggered successfully! Snapshot ID: {snapshot_id}\")\n",
        "\n",
        "        # Phase 2: Poll and retrieve results\n",
        "        scraped_content = poll_and_retrieve_results(snapshot_id, url)\n",
        "        return scraped_content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Bright Data API Error: {e}\")\n",
        "        return simple_scrape(url)\n",
        "\n",
        "def poll_and_retrieve_results(snapshot_id: str, original_url: str, max_wait_minutes: int = 5) -> ScrapedContentOutput:\n",
        "    \"\"\"\n",
        "    Poll Bright Data for results and retrieve when ready\n",
        "    \"\"\"\n",
        "    print(f\"⏳ Phase 2: Polling for results (max {max_wait_minutes} minutes)...\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {BRIGHTDATA_API_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    progress_url = f\"https://api.brightdata.com/datasets/v3/progress/{snapshot_id}\"\n",
        "    start_time = time.time()\n",
        "    max_wait_seconds = max_wait_minutes * 60\n",
        "    poll_count = 0\n",
        "\n",
        "    while True:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        poll_count += 1\n",
        "\n",
        "        if elapsed_time > max_wait_seconds:\n",
        "            print(f\"⏰ Timeout after {max_wait_minutes} minutes\")\n",
        "            return ScrapedContentOutput(\n",
        "                url=original_url,\n",
        "                html_content=\"\",\n",
        "                scrape_status=f\"timeout_after_{max_wait_minutes}min\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            print(f\"📡 Poll #{poll_count} - Checking status... ({elapsed_time:.0f}s elapsed)\")\n",
        "\n",
        "            response = requests.get(progress_url, headers=headers, timeout=30)\n",
        "\n",
        "            if response.ok:\n",
        "                status_data = response.json()\n",
        "                current_status = status_data.get('status', 'unknown')\n",
        "\n",
        "                print(f\"📊 Status: {current_status}\")\n",
        "\n",
        "                if current_status == 'done' or current_status == 'ready':\n",
        "                    print(\"✅ Scraping completed! Downloading results...\")\n",
        "                    return download_scraped_results(snapshot_id, original_url)\n",
        "\n",
        "                elif current_status == 'failed':\n",
        "                    print(\"❌ Scraping failed!\")\n",
        "                    return ScrapedContentOutput(\n",
        "                        url=original_url,\n",
        "                        html_content=\"\",\n",
        "                        scrape_status=\"failed\"\n",
        "                    )\n",
        "\n",
        "                elif current_status == 'running':\n",
        "                    print(\"⏳ Still processing... waiting 30 seconds\")\n",
        "                    time.sleep(30)\n",
        "                    continue\n",
        "\n",
        "                else:\n",
        "                    print(f\"⚠️ Unknown status: {current_status}, waiting 30 seconds\")\n",
        "                    time.sleep(30)\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "                print(f\"❌ Status check failed: {response.status_code}\")\n",
        "                if poll_count >= 3:  # Give up after 3 failed status checks\n",
        "                    break\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during polling: {e}\")\n",
        "            if poll_count >= 3:  # Give up after 3 errors\n",
        "                break\n",
        "            time.sleep(30)\n",
        "            continue\n",
        "\n",
        "    # If we get here, something went wrong\n",
        "    print(\"❌ Polling failed, using fallback scraping\")\n",
        "    return simple_scrape(original_url)\n",
        "\n",
        "def download_scraped_results(snapshot_id: str, original_url: str) -> ScrapedContentOutput:\n",
        "    \"\"\"\n",
        "    Download the actual scraped content from Bright Data\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {BRIGHTDATA_API_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Try multiple download endpoints\n",
        "    download_urls = [\n",
        "        f\"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json\",\n",
        "        f\"https://api.brightdata.com/datasets/v3/download/{snapshot_id}\",\n",
        "        f\"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}\"\n",
        "    ]\n",
        "\n",
        "    for download_url in download_urls:\n",
        "        try:\n",
        "            print(f\"📥 Trying download: {download_url}\")\n",
        "\n",
        "            response = requests.get(download_url, headers=headers, timeout=60)\n",
        "\n",
        "            if response.ok:\n",
        "                print(f\"✅ Download successful! ({len(response.content)} bytes)\")\n",
        "\n",
        "                try:\n",
        "                    # Try to parse as JSON\n",
        "                    scraped_data = response.json()\n",
        "\n",
        "                    # Extract HTML content from the data structure\n",
        "                    html_content = extract_html_from_response(scraped_data)\n",
        "\n",
        "                    return ScrapedContentOutput(\n",
        "                        url=original_url,\n",
        "                        html_content=html_content,\n",
        "                        scrape_status=\"success\"\n",
        "                    )\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    # If not JSON, treat as plain text\n",
        "                    return ScrapedContentOutput(\n",
        "                        url=original_url,\n",
        "                        html_content=response.text,\n",
        "                        scrape_status=\"success_text\"\n",
        "                    )\n",
        "\n",
        "            else:\n",
        "                print(f\"❌ Download failed: {response.status_code}\")\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Download error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # All download attempts failed\n",
        "    print(\"❌ All download attempts failed\")\n",
        "    return ScrapedContentOutput(\n",
        "        url=original_url,\n",
        "        html_content=\"\",\n",
        "        scrape_status=\"download_failed\"\n",
        "    )\n",
        "\n",
        "def extract_html_from_response(scraped_data) -> str:\n",
        "    \"\"\"\n",
        "    Extract HTML content from Bright Data response structure\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Bright Data typically returns data in different structures\n",
        "        # Try common patterns\n",
        "\n",
        "        if isinstance(scraped_data, list) and scraped_data:\n",
        "            # If it's a list, take the first item\n",
        "            item = scraped_data[0]\n",
        "\n",
        "            # Look for common HTML fields\n",
        "            html_fields = ['html', 'page_html', 'content', 'body', 'raw_html']\n",
        "            for field in html_fields:\n",
        "                if isinstance(item, dict) and field in item:\n",
        "                    return str(item[field])\n",
        "\n",
        "            # If no specific HTML field, convert the whole item to string\n",
        "            return str(item)\n",
        "\n",
        "        elif isinstance(scraped_data, dict):\n",
        "            # Look for HTML in dictionary\n",
        "            html_fields = ['html', 'page_html', 'content', 'body', 'raw_html', 'data']\n",
        "            for field in html_fields:\n",
        "                if field in scraped_data:\n",
        "                    content = scraped_data[field]\n",
        "                    if isinstance(content, list) and content:\n",
        "                        return str(content[0])\n",
        "                    return str(content)\n",
        "\n",
        "            # If no specific field, return the whole dict as string\n",
        "            return str(scraped_data)\n",
        "\n",
        "        else:\n",
        "            # Return as-is if not dict or list\n",
        "            return str(scraped_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error extracting HTML: {e}\")\n",
        "        return str(scraped_data)\n",
        "\n",
        "def simple_scrape(url: str) -> ScrapedContentOutput:\n",
        "    \"\"\"Fallback scraping using requests\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return ScrapedContentOutput(\n",
        "                url=url,\n",
        "                html_content=response.text[:5000],  # Limit content size\n",
        "                scrape_status=\"success_fallback\"\n",
        "            )\n",
        "        else:\n",
        "            return ScrapedContentOutput(\n",
        "                url=url,\n",
        "                html_content=\"\",\n",
        "                scrape_status=f\"failed_{response.status_code}\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        return ScrapedContentOutput(\n",
        "            url=url,\n",
        "            html_content=\"\",\n",
        "            scrape_status=f\"error_{str(e)}\"\n",
        "        )\n",
        "\n",
        "# Chain 1: Domain Extraction (No LLM needed)\n",
        "def domain_extraction_chain(inputs):\n",
        "    email = inputs['email']\n",
        "    result = extract_domain_from_email(email)\n",
        "    return {'domain_output': result, 'domain': result.domain}\n",
        "\n",
        "# Chain 2: Search Query Builder\n",
        "search_query_prompt = PromptTemplate(\n",
        "    input_variables=[\"domain\"],\n",
        "    template=\"\"\"\n",
        "You are tasked with creating an optimal search query to find the official website for a company domain.\n",
        "\n",
        "Domain: {domain}\n",
        "\n",
        "Create a search query that will help find the official company website. Consider:\n",
        "- The domain name itself\n",
        "- Adding terms like \"official website\" if helpful\n",
        "- Avoiding overly complex queries\n",
        "\n",
        "Return your response in this exact JSON format:\n",
        "{{\n",
        "    \"search_query\": \"your optimized search query here\",\n",
        "    \"domain\": \"{domain}\"\n",
        "}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "search_query_chain = LLMChain(\n",
        "    llm=gemini_llm,\n",
        "    prompt=search_query_prompt,\n",
        "    output_parser=SearchQueryOutputParser(),\n",
        "    output_key=\"search_query_output\"\n",
        ")\n",
        "\n",
        "# Chain 3: Serper API Call (Custom function)\n",
        "def serper_search_chain(inputs):\n",
        "    search_query_output = inputs['search_query_output']\n",
        "    query = search_query_output.search_query\n",
        "    results = call_serper_api(query)\n",
        "    return {'search_results_output': results}\n",
        "\n",
        "# Chain 4: URL Selection\n",
        "url_selection_prompt = PromptTemplate(\n",
        "    input_variables=[\"search_results\"],\n",
        "    template=\"\"\"\n",
        "You are an expert at identifying official company websites from search results.\n",
        "\n",
        "Search Results:\n",
        "{search_results}\n",
        "\n",
        "Analyze these search results and select the BEST URL that represents the official company website. Consider:\n",
        "- Official company domains vs third-party sites\n",
        "- Homepage vs subpages\n",
        "- Credibility and authority of the source\n",
        "- Relevance to the original domain\n",
        "\n",
        "Return your response in this exact JSON format:\n",
        "{{\n",
        "    \"selected_url\": \"the best URL from the results\",\n",
        "    \"reasoning\": \"brief explanation of why you chose this URL\",\n",
        "    \"confidence_score\": 0.95\n",
        "}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def format_search_results_for_prompt(search_results_output):\n",
        "    results_text = \"\"\n",
        "    for i, result in enumerate(search_results_output.results, 1):\n",
        "        results_text += f\"{i}. Title: {result.title}\\n\"\n",
        "        results_text += f\"   URL: {result.url}\\n\"\n",
        "        results_text += f\"   Snippet: {result.snippet}\\n\\n\"\n",
        "    return results_text\n",
        "\n",
        "url_selection_chain = LLMChain(\n",
        "    llm=gemini_llm,\n",
        "    prompt=url_selection_prompt,\n",
        "    output_parser=URLSelectionOutputParser(),\n",
        "    output_key=\"url_selection_output\"\n",
        ")\n",
        "\n",
        "# Chain 5: Content Scraping (Custom function)\n",
        "def content_scraping_chain(inputs):\n",
        "    url_selection_output = inputs['url_selection_output']\n",
        "    selected_url = url_selection_output.selected_url\n",
        "\n",
        "    # Extract domain root if needed\n",
        "    try:\n",
        "        from urllib.parse import urlparse\n",
        "        parsed = urlparse(selected_url)\n",
        "        root_url = f\"{parsed.scheme}://{parsed.netloc}\"\n",
        "    except:\n",
        "        root_url = selected_url\n",
        "\n",
        "    scraped_content = call_brightdata_api(root_url)\n",
        "    return {'scraped_content_output': scraped_content}\n",
        "\n",
        "# Chain 6: Summary Generation\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"scraped_content\", \"url\", \"domain\"],\n",
        "    template=\"\"\"\n",
        "You are tasked with creating a concise, one-line summary of a website based on its scraped content.\n",
        "\n",
        "Website URL: {url}\n",
        "Domain: {domain}\n",
        "Scraped Content: {scraped_content}\n",
        "\n",
        "Create a single, clear sentence that describes what this website/company does. Focus on:\n",
        "- Main business purpose or service\n",
        "- Industry or sector\n",
        "- Key value proposition\n",
        "\n",
        "Return your response in this exact JSON format:\n",
        "{{\n",
        "    \"summary\": \"One clear sentence describing what this company/website does\",\n",
        "    \"url\": \"{url}\",\n",
        "    \"domain\": \"{domain}\",\n",
        "    \"timestamp\": \"{timestamp}\"\n",
        "}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "summary_chain = LLMChain(\n",
        "    llm=gemini_llm,\n",
        "    prompt=summary_prompt,\n",
        "    output_parser=FinalSummaryOutputParser(),\n",
        "    output_key=\"final_summary\"\n",
        ")\n",
        "\n",
        "print(\"✅ All chain components created successfully!\")\n",
        "print(\"📋 Individual chains ready:\")\n",
        "print(\"   1. Domain Extraction ✓\")\n",
        "print(\"   2. Search Query Builder ✓\")\n",
        "print(\"   3. Serper API Search ✓\")\n",
        "print(\"   4. URL Selection ✓\")\n",
        "print(\"   5. Content Scraping ✓\")\n",
        "print(\"   6. Summary Generation ✓\")\n",
        "print(\"\\n🚀 Next: Run Cell 3 to create the main SequentialChain and test it!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9qCHUtwHHGV",
        "outputId": "5ce1bac0-95f7-46e3-aeba-7e70c888740c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All chain components created successfully!\n",
            "📋 Individual chains ready:\n",
            "   1. Domain Extraction ✓\n",
            "   2. Search Query Builder ✓\n",
            "   3. Serper API Search ✓\n",
            "   4. URL Selection ✓\n",
            "   5. Content Scraping ✓\n",
            "   6. Summary Generation ✓\n",
            "\n",
            "🚀 Next: Run Cell 3 to create the main SequentialChain and test it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 3: Main Sequential Chain and Execution\n",
        "\n",
        "from langchain.chains.base import Chain\n",
        "from typing import Dict, Any, List, ClassVar\n",
        "\n",
        "# Custom wrapper chains to integrate functions with LangChain\n",
        "class DomainExtractionChain(Chain):\n",
        "    \"\"\"Custom chain for domain extraction\"\"\"\n",
        "\n",
        "    input_keys: ClassVar[List[str]] = [\"email\"]\n",
        "    output_keys: ClassVar[List[str]] = [\"domain_output\", \"domain\"]\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        return domain_extraction_chain(inputs)\n",
        "\n",
        "class SerperSearchChain(Chain):\n",
        "    \"\"\"Custom chain for Serper API search\"\"\"\n",
        "\n",
        "    input_keys: ClassVar[List[str]] = [\"search_query_output\"]\n",
        "    output_keys: ClassVar[List[str]] = [\"search_results_output\"]\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        return serper_search_chain(inputs)\n",
        "\n",
        "class ContentScrapingChain(Chain):\n",
        "    \"\"\"Custom chain for content scraping\"\"\"\n",
        "\n",
        "    input_keys: ClassVar[List[str]] = [\"url_selection_output\"]\n",
        "    output_keys: ClassVar[List[str]] = [\"scraped_content_output\"]\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        return content_scraping_chain(inputs)\n",
        "\n",
        "# Custom chain to format search results for URL selection\n",
        "class URLSelectionPreprocessChain(Chain):\n",
        "    \"\"\"Preprocess search results for URL selection\"\"\"\n",
        "\n",
        "    input_keys: ClassVar[List[str]] = [\"search_results_output\"]\n",
        "    output_keys: ClassVar[List[str]] = [\"search_results\"]\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        search_results_output = inputs['search_results_output']\n",
        "        formatted_results = format_search_results_for_prompt(search_results_output)\n",
        "        return {'search_results': formatted_results}\n",
        "\n",
        "# Custom chain to format scraped content for summary\n",
        "class SummaryPreprocessChain(Chain):\n",
        "    \"\"\"Preprocess scraped content for summary generation\"\"\"\n",
        "\n",
        "    input_keys: ClassVar[List[str]] = [\"scraped_content_output\", \"url_selection_output\", \"domain\"]\n",
        "    output_keys: ClassVar[List[str]] = [\"scraped_content\", \"url\", \"timestamp\"]\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        scraped_output = inputs['scraped_content_output']\n",
        "        url_output = inputs['url_selection_output']\n",
        "        domain = inputs['domain']\n",
        "\n",
        "        # Limit content size for processing\n",
        "        content = scraped_output.html_content[:2000] + \"...\" if len(scraped_output.html_content) > 2000 else scraped_output.html_content\n",
        "\n",
        "        return {\n",
        "            'scraped_content': content,\n",
        "            'url': url_output.selected_url,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Create the main sequential chain\n",
        "def create_email_to_summary_chain():\n",
        "    \"\"\"Create the main sequential chain\"\"\"\n",
        "\n",
        "    chains = [\n",
        "        DomainExtractionChain(),           # email → domain\n",
        "        search_query_chain,                # domain → search_query\n",
        "        SerperSearchChain(),               # search_query → search_results\n",
        "        URLSelectionPreprocessChain(),     # search_results → formatted_results\n",
        "        url_selection_chain,               # formatted_results → selected_url\n",
        "        ContentScrapingChain(),            # selected_url → scraped_content\n",
        "        SummaryPreprocessChain(),          # scraped_content → formatted_content\n",
        "        summary_chain                      # formatted_content → final_summary\n",
        "    ]\n",
        "\n",
        "    sequential_chain = SequentialChain(\n",
        "        chains=chains,\n",
        "        input_variables=[\"email\"],\n",
        "        output_variables=[\"final_summary\", \"domain\", \"url_selection_output\", \"scraped_content_output\"],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return sequential_chain\n",
        "\n",
        "# Main execution function\n",
        "def analyze_email_domain(email: str):\n",
        "    \"\"\"\n",
        "    Main function to analyze an email domain and generate a website summary\n",
        "\n",
        "    Args:\n",
        "        email (str): Email address (e.g., \"name@company.com\")\n",
        "\n",
        "    Returns:\n",
        "        dict: Complete analysis results\n",
        "    \"\"\"\n",
        "    print(f\"🚀 Starting analysis for: {email}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"⏳ This process may take 3-5 minutes due to Bright Data scraping...\")\n",
        "    print(\"📋 Steps: Domain Extract → Search → Select → Scrape (with polling) → Summarize\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Create and run the chain\n",
        "        main_chain = create_email_to_summary_chain()\n",
        "\n",
        "        # Execute the chain\n",
        "        start_time = time.time()\n",
        "        result = main_chain({\"email\": email})\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # Extract and display results\n",
        "        final_summary = result['final_summary']\n",
        "        domain = result['domain']\n",
        "        url_selection = result['url_selection_output']\n",
        "        scraped_content = result['scraped_content_output']\n",
        "\n",
        "        print(f\"\\n✅ Analysis Complete! (took {total_time:.1f} seconds)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"📧 Original Email: {email}\")\n",
        "        print(f\"🌐 Extracted Domain: {domain}\")\n",
        "        print(f\"🔗 Selected URL: {url_selection.selected_url}\")\n",
        "        print(f\"🤖 Scraping Status: {scraped_content.scrape_status}\")\n",
        "        print(f\"📝 Website Summary: {final_summary.summary}\")\n",
        "        print(f\"🎯 Confidence Score: {url_selection.confidence_score}\")\n",
        "        print(f\"💭 Selection Reasoning: {url_selection.reasoning}\")\n",
        "        print(f\"⏰ Completed: {final_summary.timestamp}\")\n",
        "\n",
        "        return {\n",
        "            'email': email,\n",
        "            'domain': domain,\n",
        "            'selected_url': url_selection.selected_url,\n",
        "            'summary': final_summary.summary,\n",
        "            'confidence': url_selection.confidence_score,\n",
        "            'reasoning': url_selection.reasoning,\n",
        "            'scrape_status': scraped_content.scrape_status,\n",
        "            'content_length': len(scraped_content.html_content),\n",
        "            'processing_time': total_time,\n",
        "            'timestamp': final_summary.timestamp,\n",
        "            'full_results': result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during analysis: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\n",
        "            'email': email,\n",
        "            'error': str(e),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Test function with multiple emails\n",
        "def test_multiple_emails(emails: List[str]):\n",
        "    \"\"\"Test the chain with multiple email addresses\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for email in emails:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Testing: {email}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        result = analyze_email_domain(email)\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"\\n⏳ Waiting 2 seconds before next request...\")\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage and test cases\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🔧 Email to Website Summary Chain Ready!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"📋 Available functions:\")\n",
        "    print(\"1. analyze_email_domain(email) - Analyze single email\")\n",
        "    print(\"2. test_multiple_emails([emails]) - Test multiple emails\")\n",
        "    print()\n",
        "    print(\"⚠️  Before running, make sure to:\")\n",
        "    print(\"   - Replace SERPER_API_KEY with your actual Serper.dev API key\")\n",
        "    print(\"   - Replace GOOGLE_API_KEY with your actual Google AI API key\")\n",
        "    print(\"   - Bright Data token is already configured\")\n",
        "    print()\n",
        "    print(\"🚀 Example usage:\")\n",
        "    print(\"   result = analyze_email_domain('contact@gemengserv.com')\")\n",
        "    print()\n",
        "    print(\"📊 Test with multiple emails:\")\n",
        "    print(\"   test_emails = ['contact@gemengserv.com', 'info@example.com']\")\n",
        "    print(\"   results = test_multiple_emails(test_emails)\")\n",
        "\n",
        "    # Uncomment to run a quick test (after adding your API keys):\n",
        "    # result = analyze_email_domain('contact@gemengserv.com')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ Setup complete! Ready to analyze email domains.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkyjR69YHKIH",
        "outputId": "8274214b-3711-4363-fddd-eb46b84cd9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Email to Website Summary Chain Ready!\n",
            "============================================================\n",
            "📋 Available functions:\n",
            "1. analyze_email_domain(email) - Analyze single email\n",
            "2. test_multiple_emails([emails]) - Test multiple emails\n",
            "\n",
            "⚠️  Before running, make sure to:\n",
            "   - Replace SERPER_API_KEY with your actual Serper.dev API key\n",
            "   - Replace GOOGLE_API_KEY with your actual Google AI API key\n",
            "   - Bright Data token is already configured\n",
            "\n",
            "🚀 Example usage:\n",
            "   result = analyze_email_domain('contact@gemengserv.com')\n",
            "\n",
            "📊 Test with multiple emails:\n",
            "   test_emails = ['contact@gemengserv.com', 'info@example.com']\n",
            "   results = test_multiple_emails(test_emails)\n",
            "\n",
            "============================================================\n",
            "✅ Setup complete! Ready to analyze email domains.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = analyze_email_domain('contact@gemengserv.com')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "9EJqoRs7Hatj",
        "outputId": "3f0316ba-6139-45e5-b32e-3c8b21f35183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting analysis for: contact@gemengserv.com\n",
            "============================================================\n",
            "⏳ This process may take 3-5 minutes due to Bright Data scraping...\n",
            "📋 Steps: Domain Extract → Search → Select → Scrape (with polling) → Summarize\n",
            "============================================================\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "🔍 Searching Serper API: 'gemengserv.com official website'\n",
            "✅ Found 5 results\n",
            "\n",
            "🤖 Starting Bright Data scraping for: https://gemengserv.com\n",
            "⏳ Phase 1: Triggering scraping job...\n",
            "✅ Job triggered successfully! Snapshot ID: s_mfbc01331v88vjuqdd\n",
            "⏳ Phase 2: Polling for results (max 5 minutes)...\n",
            "📡 Poll #1 - Checking status... (0s elapsed)\n",
            "📊 Status: running\n",
            "⏳ Still processing... waiting 30 seconds\n",
            "📡 Poll #2 - Checking status... (31s elapsed)\n",
            "📊 Status: running\n",
            "⏳ Still processing... waiting 30 seconds\n",
            "📡 Poll #3 - Checking status... (61s elapsed)\n",
            "📊 Status: running\n",
            "⏳ Still processing... waiting 30 seconds\n",
            "📡 Poll #4 - Checking status... (92s elapsed)\n",
            "📊 Status: running\n",
            "⏳ Still processing... waiting 30 seconds\n",
            "📡 Poll #5 - Checking status... (122s elapsed)\n",
            "📊 Status: running\n",
            "⏳ Still processing... waiting 30 seconds\n",
            "📡 Poll #6 - Checking status... (153s elapsed)\n",
            "📊 Status: ready\n",
            "✅ Scraping completed! Downloading results...\n",
            "📥 Trying download: https://api.brightdata.com/datasets/v3/snapshot/s_mfbc01331v88vjuqdd?format=json\n",
            "✅ Download successful! (393989 bytes)\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "✅ Analysis Complete! (took 168.8 seconds)\n",
            "============================================================\n",
            "📧 Original Email: contact@gemengserv.com\n",
            "🌐 Extracted Domain: gemengserv.com\n",
            "🔗 Selected URL: https://gemengserv.com/\n",
            "🤖 Scraping Status: success\n",
            "📝 Website Summary: ```json\n",
            "{\n",
            "  \"summary\": \"GEM Engserv is a construction project management consultancy providing pre-, during-, and post-construction services.\",\n",
            "  \"url\": \"https://gemengserv.com/\",\n",
            "  \"domain\": \"gemengserv.com\",\n",
            "  \"timestamp\": \"2025-09-08T16:26:25.551413\"\n",
            "}\n",
            "```\n",
            "🎯 Confidence Score: 0.98\n",
            "💭 Selection Reasoning: This URL is the homepage of the GEM Engserv website.  It's the root domain and provides a comprehensive overview of the company's services. The other URLs are subpages, indicating they belong to the same website, but the homepage is the most authoritative and representative of the entire company's online presence.\n",
            "⏰ Completed: 2025-09-08T16:26:29.525496\n"
          ]
        }
      ]
    }
  ]
}