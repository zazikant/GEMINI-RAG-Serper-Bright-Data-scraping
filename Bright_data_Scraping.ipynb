{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yKzee7HxuuGm",
        "outputId": "f297c858-15e6-4864-ef84-eee2230ef26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-brightdata\n",
            "  Downloading langchain_brightdata-0.1.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.56 in /usr/local/lib/python3.12/dist-packages (from langchain-brightdata) (0.3.75)\n",
            "Requirement already satisfied: pydantic>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from langchain-brightdata) (2.11.7)\n",
            "Requirement already satisfied: requests>=2.32.0 in /usr/local/lib/python3.12/dist-packages (from langchain-brightdata) (2.32.4)\n",
            "Requirement already satisfied: aiohttp>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from langchain-brightdata) (3.12.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.0->langchain-brightdata) (1.20.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.56->langchain-brightdata) (0.4.23)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.56->langchain-brightdata) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.56->langchain-brightdata) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.56->langchain-brightdata) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.56->langchain-brightdata) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.56->langchain-brightdata) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.3->langchain-brightdata) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.3->langchain-brightdata) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.3->langchain-brightdata) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->langchain-brightdata) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->langchain-brightdata) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->langchain-brightdata) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->langchain-brightdata) (2025.8.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.56->langchain-brightdata) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.56->langchain-brightdata) (1.3.1)\n",
            "Downloading langchain_brightdata-0.1.3-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: langchain-brightdata\n",
            "Successfully installed langchain-brightdata-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-brightdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def test_brightdata_direct_api(url, dataset_id, api_key):\n",
        "    \"\"\"\n",
        "    Test Bright Data's direct API endpoint (not LangChain)\n",
        "    \"\"\"\n",
        "    # API endpoints\n",
        "    scrape_endpoint = f\"https://api.brightdata.com/datasets/v3/scrape\"\n",
        "    trigger_endpoint = f\"https://api.brightdata.com/datasets/v3/trigger\"\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {api_key}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    payload = [{\"url\": url}]\n",
        "\n",
        "    print(f\"🚀 Testing Bright Data Direct API\")\n",
        "    print(f\"🔗 URL: {url}\")\n",
        "    print(f\"📊 Dataset ID: {dataset_id}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Method 1: Try synchronous scraping\n",
        "        print(\"\\n🧪 Method 1: Synchronous Scraping\")\n",
        "        params = {\n",
        "            'dataset_id': dataset_id,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        print(\"⏳ Sending request...\")\n",
        "        response = requests.post(\n",
        "            scrape_endpoint,\n",
        "            headers=headers,\n",
        "            params=params,\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        print(f\"📊 Status Code: {response.status_code}\")\n",
        "        print(f\"📏 Response Size: {len(response.content)} bytes\")\n",
        "\n",
        "        if response.ok:\n",
        "            data = response.json()\n",
        "            print(\"✅ Synchronous scraping successful!\")\n",
        "            return save_results(url, dataset_id, data, \"synchronous\")\n",
        "        else:\n",
        "            print(f\"❌ Synchronous failed: {response.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Synchronous error: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        # Method 2: Try asynchronous trigger\n",
        "        print(\"\\n🧪 Method 2: Asynchronous Trigger\")\n",
        "        params = {\n",
        "            'dataset_id': dataset_id,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        print(\"⏳ Triggering collection...\")\n",
        "        response = requests.post(\n",
        "            trigger_endpoint,\n",
        "            headers=headers,\n",
        "            params=params,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        print(f\"📊 Status Code: {response.status_code}\")\n",
        "\n",
        "        if response.ok:\n",
        "            result = response.json()\n",
        "            print(\"✅ Collection triggered successfully!\")\n",
        "            print(f\"📄 Response: {result}\")\n",
        "\n",
        "            if 'snapshot_id' in result:\n",
        "                snapshot_id = result['snapshot_id']\n",
        "                print(f\"📋 Snapshot ID: {snapshot_id}\")\n",
        "                print(\"💡 Use this ID to retrieve results later\")\n",
        "                return save_results(url, dataset_id, result, \"asynchronous\")\n",
        "            else:\n",
        "                return save_results(url, dataset_id, result, \"trigger_response\")\n",
        "        else:\n",
        "            print(f\"❌ Trigger failed: {response.text}\")\n",
        "            return save_results(url, dataset_id, {\"error\": response.text}, \"error\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Trigger error: {str(e)}\")\n",
        "        return save_results(url, dataset_id, {\"error\": str(e)}, \"error\")\n",
        "\n",
        "def save_results(url, dataset_id, data, method):\n",
        "    \"\"\"\n",
        "    Save results to JSON file\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    url_clean = url.replace('https://', '').replace('http://', '').replace('/', '_').replace('.', '_')\n",
        "    filename = f\"brightdata_{method}_{url_clean}_{timestamp}.json\"\n",
        "\n",
        "    result_data = {\n",
        "        \"url\": url,\n",
        "        \"dataset_id\": dataset_id,\n",
        "        \"method\": method,\n",
        "        \"scraped_at\": datetime.now().isoformat(),\n",
        "        \"scraper\": \"Bright Data Direct API\",\n",
        "        \"data\": data\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"💾 Results saved to: {filename}\")\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to save: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def test_dataset_info(dataset_id, api_key):\n",
        "    \"\"\"\n",
        "    Try to get information about the dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 Testing Dataset Info: {dataset_id}\")\n",
        "\n",
        "    # Try different info endpoints\n",
        "    info_endpoints = [\n",
        "        f\"https://api.brightdata.com/datasets/v3/dataset/{dataset_id}\",\n",
        "        f\"https://api.brightdata.com/datasets/v3/info?dataset_id={dataset_id}\",\n",
        "        f\"https://api.brightdata.com/datasets/{dataset_id}/info\"\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {api_key}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    for endpoint in info_endpoints:\n",
        "        try:\n",
        "            print(f\"📡 Trying: {endpoint}\")\n",
        "            response = requests.get(endpoint, headers=headers, timeout=10)\n",
        "            print(f\"   Status: {response.status_code}\")\n",
        "\n",
        "            if response.ok:\n",
        "                info = response.json()\n",
        "                print(f\"   ✅ Dataset info retrieved!\")\n",
        "                print(f\"   📄 Info: {json.dumps(info, indent=2)}\")\n",
        "                return info\n",
        "            else:\n",
        "                print(f\"   ❌ {response.text[:100]}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error: {str(e)}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to test Bright Data\n",
        "    \"\"\"\n",
        "    # Configuration\n",
        "    URL = \"https://gemengserv.com\"\n",
        "    DATASET_ID = \"gd_m6gjtfmeh43we6cqc\"  # From your original URL\n",
        "    API_KEY = \"\"\n",
        "\n",
        "    print(\"🌟 Bright Data Direct API Test\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Test dataset info first\n",
        "    dataset_info = test_dataset_info(DATASET_ID, API_KEY)\n",
        "\n",
        "    # Test scraping\n",
        "    result_file = test_brightdata_direct_api(URL, DATASET_ID, API_KEY)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📋 TEST SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if result_file:\n",
        "        print(f\"✅ Test completed - check {result_file}\")\n",
        "        print(\"\\n💡 Next steps:\")\n",
        "        print(\"   - If you got a snapshot_id, wait a few minutes then retrieve results\")\n",
        "        print(\"   - Check your Bright Data dashboard for collection status\")\n",
        "        print(\"   - Verify your dataset supports the target website\")\n",
        "    else:\n",
        "        print(\"❌ Test failed - no results saved\")\n",
        "        print(\"\\n🔧 Troubleshooting:\")\n",
        "        print(\"   - Verify your API key is correct\")\n",
        "        print(\"   - Check if dataset_id is valid for your account\")\n",
        "        print(\"   - Ensure the website is supported by your dataset\")\n",
        "        print(\"   - Check if you have sufficient credits\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pURm61yE7gLo",
        "outputId": "cbab7ea6-70eb-463f-eae7-99045bf77307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌟 Bright Data Direct API Test\n",
            "============================================================\n",
            "\n",
            "🔍 Testing Dataset Info: gd_m6gjtfmeh43we6cqc\n",
            "📡 Trying: https://api.brightdata.com/datasets/v3/dataset/gd_m6gjtfmeh43we6cqc\n",
            "   Status: 404\n",
            "   ❌ <!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "<meta charset=\"utf-8\">\n",
            "<title>Error</title>\n",
            "</head>\n",
            "<body>\n",
            "<\n",
            "📡 Trying: https://api.brightdata.com/datasets/v3/info?dataset_id=gd_m6gjtfmeh43we6cqc\n",
            "   Status: 404\n",
            "   ❌ <!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "<meta charset=\"utf-8\">\n",
            "<title>Error</title>\n",
            "</head>\n",
            "<body>\n",
            "<\n",
            "📡 Trying: https://api.brightdata.com/datasets/gd_m6gjtfmeh43we6cqc/info\n",
            "   Status: 404\n",
            "   ❌ <!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "<meta charset=\"utf-8\">\n",
            "<title>Error</title>\n",
            "</head>\n",
            "<body>\n",
            "<\n",
            "🚀 Testing Bright Data Direct API\n",
            "🔗 URL: https://gemengserv.com\n",
            "📊 Dataset ID: gd_m6gjtfmeh43we6cqc\n",
            "============================================================\n",
            "\n",
            "🧪 Method 1: Synchronous Scraping\n",
            "⏳ Sending request...\n",
            "❌ Synchronous error: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=60)\n",
            "\n",
            "🧪 Method 2: Asynchronous Trigger\n",
            "⏳ Triggering collection...\n",
            "📊 Status Code: 200\n",
            "✅ Collection triggered successfully!\n",
            "📄 Response: {'snapshot_id': 's_mfba6mg9u5klm6dae'}\n",
            "📋 Snapshot ID: s_mfba6mg9u5klm6dae\n",
            "💡 Use this ID to retrieve results later\n",
            "💾 Results saved to: brightdata_asynchronous_gemengserv_com_20250908_153258.json\n",
            "\n",
            "============================================================\n",
            "📋 TEST SUMMARY\n",
            "============================================================\n",
            "✅ Test completed - check brightdata_asynchronous_gemengserv_com_20250908_153258.json\n",
            "\n",
            "💡 Next steps:\n",
            "   - If you got a snapshot_id, wait a few minutes then retrieve results\n",
            "   - Check your Bright Data dashboard for collection status\n",
            "   - Verify your dataset supports the target website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run above script first and wait for snapshot id to generate. Then run below script"
      ],
      "metadata": {
        "id": "Ky5-KaDf_Ceo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def retrieve_scraping_results(snapshot_id, api_key, max_wait_minutes=10):\n",
        "    \"\"\"\n",
        "    Retrieve results from Bright Data using snapshot ID\n",
        "    \"\"\"\n",
        "    print(f\"🔍 Retrieving results for snapshot: {snapshot_id}\")\n",
        "    print(f\"⏰ Started at: {datetime.now()}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # API endpoints for retrieving results\n",
        "    download_endpoint = f\"https://api.brightdata.com/datasets/v3/progress/{snapshot_id}\"\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {api_key}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    start_time = time.time()\n",
        "    max_wait_seconds = max_wait_minutes * 60\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "            if elapsed_time > max_wait_seconds:\n",
        "                print(f\"⏰ Timeout after {max_wait_minutes} minutes\")\n",
        "                break\n",
        "\n",
        "            print(f\"📡 Checking status... ({elapsed_time:.0f}s elapsed)\")\n",
        "\n",
        "            # Check progress/status\n",
        "            response = requests.get(download_endpoint, headers=headers, timeout=30)\n",
        "\n",
        "            if response.ok:\n",
        "                status_data = response.json()\n",
        "                print(f\"📊 Status: {json.dumps(status_data, indent=2)}\")\n",
        "\n",
        "                # Check if collection is complete\n",
        "                if 'status' in status_data:\n",
        "                    if status_data['status'] == 'running':\n",
        "                        print(\"⏳ Still processing... waiting 30 seconds\")\n",
        "                        time.sleep(30)\n",
        "                        continue\n",
        "                    elif status_data['status'] == 'done':\n",
        "                        print(\"✅ Collection completed!\")\n",
        "                        break\n",
        "                    elif status_data['status'] == 'failed':\n",
        "                        print(\"❌ Collection failed!\")\n",
        "                        return save_results(snapshot_id, status_data, \"failed\")\n",
        "\n",
        "                # Try to download results anyway\n",
        "                print(\"🔄 Attempting to download results...\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(f\"❌ Status check failed: {response.status_code} - {response.text}\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error checking status: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    # Try to download the actual data\n",
        "    try:\n",
        "        print(\"\\n📥 Downloading scraped data...\")\n",
        "\n",
        "        # Try different download endpoints\n",
        "        download_urls = [\n",
        "            f\"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json\",\n",
        "            f\"https://api.brightdata.com/datasets/v3/download/{snapshot_id}\",\n",
        "            f\"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}\"\n",
        "        ]\n",
        "\n",
        "        for download_url in download_urls:\n",
        "            print(f\"🔗 Trying: {download_url}\")\n",
        "\n",
        "            response = requests.get(download_url, headers=headers, timeout=60)\n",
        "\n",
        "            if response.ok:\n",
        "                print(f\"✅ Download successful! ({len(response.content)} bytes)\")\n",
        "\n",
        "                # Try to parse as JSON\n",
        "                try:\n",
        "                    scraped_data = response.json()\n",
        "                    print(\"📄 JSON data retrieved successfully!\")\n",
        "                    return save_results(snapshot_id, scraped_data, \"completed\")\n",
        "                except json.JSONDecodeError:\n",
        "                    # Save as text if not JSON\n",
        "                    print(\"📄 Non-JSON data retrieved\")\n",
        "                    return save_results(snapshot_id, response.text, \"completed_text\")\n",
        "\n",
        "            else:\n",
        "                print(f\"   ❌ Failed: {response.status_code} - {response.text[:100]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Download error: {str(e)}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_results(snapshot_id, data, status):\n",
        "    \"\"\"\n",
        "    Save retrieved results to JSON file\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"brightdata_results_{snapshot_id}_{status}_{timestamp}.json\"\n",
        "\n",
        "    result_data = {\n",
        "        \"snapshot_id\": snapshot_id,\n",
        "        \"status\": status,\n",
        "        \"retrieved_at\": datetime.now().isoformat(),\n",
        "        \"data\": data\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"💾 Results saved to: {filename}\")\n",
        "\n",
        "        # Show preview of data\n",
        "        if isinstance(data, dict) and 'data' in str(data):\n",
        "            print(\"\\n📋 Data Preview:\")\n",
        "            print(json.dumps(data, indent=2)[:500] + \"...\" if len(str(data)) > 500 else json.dumps(data, indent=2))\n",
        "        elif isinstance(data, list) and data:\n",
        "            print(f\"\\n📋 Retrieved {len(data)} items\")\n",
        "            if isinstance(data[0], dict):\n",
        "                print(f\"📄 Sample item keys: {list(data[0].keys())}\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to save: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to retrieve results\n",
        "    \"\"\"\n",
        "    # Configuration\n",
        "    SNAPSHOT_ID = \"s_mfba6mg9u5klm6dae\"  # From your previous run\n",
        "    API_KEY = \"a2a22824d35a919cfc9955980b9bcf1f9d92d70fc54da6229d713edc3825efb6\"\n",
        "\n",
        "    print(\"🌟 Bright Data Results Retriever\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Retrieve results\n",
        "    result_file = retrieve_scraping_results(SNAPSHOT_ID, API_KEY, max_wait_minutes=5)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📋 RETRIEVAL SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if result_file:\n",
        "        print(f\"✅ Results retrieved successfully!\")\n",
        "        print(f\"📄 Check file: {result_file}\")\n",
        "        print(f\"\\n🎉 You now have the scraped data from gemengserv.com!\")\n",
        "    else:\n",
        "        print(\"❌ Failed to retrieve results\")\n",
        "        print(\"\\n💡 Troubleshooting:\")\n",
        "        print(\"   - The scraping might still be in progress\")\n",
        "        print(\"   - Check your Bright Data dashboard\")\n",
        "        print(\"   - Try again in a few minutes\")\n",
        "        print(f\"   - Use snapshot ID: {SNAPSHOT_ID}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0a50EG68Q8E",
        "outputId": "3abdb65d-2eed-481a-edcb-a511d578aeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌟 Bright Data Results Retriever\n",
            "============================================================\n",
            "🔍 Retrieving results for snapshot: s_mfba6mg9u5klm6dae\n",
            "⏰ Started at: 2025-09-08 15:38:52.872854\n",
            "============================================================\n",
            "📡 Checking status... (0s elapsed)\n",
            "📊 Status: {\n",
            "  \"status\": \"ready\",\n",
            "  \"snapshot_id\": \"s_mfba6mg9u5klm6dae\",\n",
            "  \"dataset_id\": \"gd_m6gjtfmeh43we6cqc\",\n",
            "  \"records\": 1,\n",
            "  \"errors\": 0,\n",
            "  \"collection_duration\": 53411\n",
            "}\n",
            "🔄 Attempting to download results...\n",
            "\n",
            "📥 Downloading scraped data...\n",
            "🔗 Trying: https://api.brightdata.com/datasets/v3/snapshot/s_mfba6mg9u5klm6dae?format=json\n",
            "✅ Download successful! (384284 bytes)\n",
            "📄 JSON data retrieved successfully!\n",
            "💾 Results saved to: brightdata_results_s_mfba6mg9u5klm6dae_completed_20250908_153855.json\n",
            "\n",
            "📋 Retrieved 1 items\n",
            "📄 Sample item keys: ['markdown', 'url', 'html2text', 'page_html', 'ld_json', 'page_title', 'timestamp', 'input']\n",
            "\n",
            "============================================================\n",
            "📋 RETRIEVAL SUMMARY\n",
            "============================================================\n",
            "✅ Results retrieved successfully!\n",
            "📄 Check file: brightdata_results_s_mfba6mg9u5klm6dae_completed_20250908_153855.json\n",
            "\n",
            "🎉 You now have the scraped data from gemengserv.com!\n"
          ]
        }
      ]
    }
  ]
}